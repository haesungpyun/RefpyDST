{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haesungpyun/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/haesungpyun/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from my_funcs import (\n",
    "    default_transformation, read_json_from_data_dir,\n",
    "    normalize, embed_query_retrieve_examples,\n",
    "    make_two_type_msg, get_python_chat_prompt, \n",
    "    parse_python_completion, update_dialogue_state, \n",
    "    compute_acc, calculate_token_f1, evaluate,\n",
    "    DataOntologyNormalizer, Ontology,\n",
    "    copy, defaultdict, random,\n",
    "    openai, tiktoken, SentenceTransformer\n",
    ")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from my_openai_key import get_openai_key\n",
    "openai.api_key = get_openai_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data to get a query and train data to retrieve examples\n",
    "with open(\"data/mw24_100p_dev_100_sampled.json\", 'r') as f:\n",
    "    dev_dataset = json.load(f)\n",
    "with open('data/mw21_1p_train_v1.json', 'r') as f:\n",
    "    train_dataset = json.load(f)\n",
    "    \n",
    "corpus = list(map(default_transformation, train_dataset))\n",
    "\n",
    "word_tokenized_corpus = []\n",
    "for sent in corpus:\n",
    "    tokens = sent.split()\n",
    "    word_tokenized_corpus.append(tokens)\n",
    "\n",
    "bm25 = BM25Okapi(word_tokenized_corpus[:5])\n",
    "\n",
    "query = default_transformation(dev_dataset[100])\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "score_idx_dict = {score:i for i, score in enumerate(bm25.get_scores(tokenized_query))}\n",
    "sorted_scores = sorted(score_idx_dict, reverse=True)\n",
    "# topk_list = [train_dataset[idx] for idx in [score_dict[score] for score in sorted_scores[:5]]]\n",
    "k = 2\n",
    "query_result = [score_idx_dict[top_score] for top_score in sorted_scores[:k]]\n",
    "\n",
    "np.array([query_result]), np.array([sorted_scores[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data to get a query and train data to retrieve examples\n",
    "dev_dataset = read_json_from_data_dir(\"data/mw24_100p_dev_100_sampled.json\")\n",
    "train_dataset = read_json_from_data_dir('/home/haesungpyun/my_refpydst/data/mw21_1p_train_v1.json')\n",
    "\n",
    "# Register all dialogues from the train dataset to example pool and Get all the unique dialogue ids in example pool\n",
    "example_pool = []\n",
    "selected_dialog_id_from_split = set()\n",
    "for dataset in [train_dataset]:\n",
    "    example_pool += dataset\n",
    "    selected_dialog_id_from_split.update([dial['ID'] for dial in dataset])\n",
    "\n",
    "# Load the all train data index\n",
    "retriever_full_path = '/home/haesungpyun/my_refpydst/outputs/runs/retriever/mw21_1p_train/referred_states/split_v1'\n",
    "search_index_filename = os.path.join(retriever_full_path, \"train_index.npy\")\n",
    "search_embeddings = np.load(search_index_filename, allow_pickle=True).item()    # {'MUL0720.json_turn_10': np.array([0.1, 0.2, ...]), ...}\n",
    "\n",
    "# Keep only embeddings for the selected dialogues in split version\n",
    "emb_dict = {k: v for k, v in search_embeddings.items() if k.split('_')[0] in selected_dialog_id_from_split}\n",
    "emb_keys = list(emb_dict.keys())\n",
    "emb_dim = emb_dict[emb_keys[0]].shape[-1]\n",
    "\n",
    "# Convert embeddings to array and Normalize them\n",
    "emb_values = np.zeros((len(emb_keys), emb_dim))\n",
    "for i, k in enumerate(emb_keys):\n",
    "    emb_values[i] = emb_dict[k]\n",
    "emb_values = normalize(emb_values)\n",
    "\n",
    "# Create a label to index mapping  {'MUL0720.json_turn_10': 1, ...} \n",
    "label_to_idx = {k: i for i, k in enumerate(emb_keys)}\n",
    "\n",
    "# Load the model for embed query\n",
    "embedder = SentenceTransformer('/home/haesungpyun/my_refpydst/outputs/runs/retriever/mw21_1p_train/referred_states/split_v1')\n",
    "\n",
    "# Tokenizer\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query에 대해 retrieve & generate & evaluate\n",
    "\n",
    "short_sys_msg = \"You are an expert in Dialogue State Tracking(DST) and python coding.\\n\"\n",
    "\n",
    "sys_msg_dict = {\"short_sys_msg\": short_sys_msg}\n",
    "\n",
    "total_log = []\n",
    "retrieving_samples = True\n",
    "num_retrieved_examples = 100\n",
    "n_smapled_examples = 10\n",
    "\n",
    "random.seed(42)\n",
    "# Randomly select a query data and Retrieve examples from example pool (train data)\n",
    "# query_data = dev_dataset[random.randint(0, len(dev_dataset))]\n",
    "for query_data in dev_dataset:\n",
    "    if retrieving_samples:\n",
    "        retrieved_examples = embed_query_retrieve_examples(\n",
    "            embedder, example_pool, query_data, \n",
    "            emb_keys, emb_values, label_to_idx, num_retrieved_examples=num_retrieved_examples)\n",
    "    else:\n",
    "        random.seed(42)\n",
    "        retrieved_examples = [example_pool[random.randint(0, len(example_pool))] for _ in range(num_retrieved_examples)]\n",
    "\n",
    "    msg_chat, gold_python = get_python_chat_prompt(query_data, retrieved_examples, system_msg)    \n",
    "    msg_chat_usr_last, msg_one_prompt = make_two_type_msg(msg_chat)\n",
    "    # raise ValueError    \n",
    "    log = defaultdict(dict)\n",
    "    examples_list = []\n",
    "    for idx, example in enumerate(retrieved_examples):\n",
    "        tmp = {}\n",
    "        tmp['ID_turn-id'] = f\"{example['ID']}-{example['turn_id']}\"\n",
    "        tmp['last_slot_values'], tmp['turn_slot_values'], tmp['slot_values'] = \\\n",
    "            example['last_slot_values'], example['turn_slot_values'], example['slot_values']\n",
    "        examples_list.append(tmp)\n",
    "    log[\"retrieve_example\"] = examples_list\n",
    "\n",
    "    log[\"query\"]['ID-turn-id']= f\"{query_data['ID']}-{query_data['turn_id']}\"\n",
    "    log[\"query\"]['last_slot_values'] = query_data['last_slot_values'] \n",
    "    log[\"query\"]['turn_slot_values'] = query_data['turn_slot_values']\n",
    "    log[\"query\"]['slot_values']= query_data['slot_values']\n",
    "    log[\"query\"]['gold-python'] = gold_python\n",
    "\n",
    "    for name, msg in {'chat_ass_last': msg_chat,'chat_user_last': msg_chat_usr_last, 'one_prompt': msg_one_prompt}.items():\n",
    "        args = {\n",
    "            \"model\": 'gpt-3.5-turbo',\n",
    "            \"messages\": msg,\n",
    "            \"max_tokens\": 120,\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop\": ['--', '\\n', ';', '#'],\n",
    "            \"n\": 2,\n",
    "            \"logprobs\": True,  # 1 needed to get back log-probabilities at all, in choice['logprobs']['token_logprobs']\n",
    "        }\n",
    "        # results = openai.chat.completions.create(**args)\n",
    "        result_list = []\n",
    "        for i, result in enumerate(results.choices):\n",
    "            tmp = {}\n",
    "            tmp['gold'] = gold_python\n",
    "            tmp['pred'] = result.message.content\n",
    "            tmp['len-normalize-logprob'] = sum(i.logprob for i in result.logprobs.content)/len(result.logprobs.content)\n",
    "            tmp['token_f1'] = calculate_token_f1(encoding, gold_python, tmp['pred'])\n",
    "            # tmp[f'toke-logprobs-{i}'] = [i.logprob for i in result.logprobs.content]\n",
    "            result_list.append(tmp)\n",
    "        sorted_result_list = sorted(result_list, key=lambda x: x['token_f1'], reverse=True)\n",
    "        log[name] = sorted_result_list\n",
    "    total_log.append(log)\n",
    "\n",
    "import json, os\n",
    "output_file = os.getcwd()+f'/tmp_{sys_msg_key}.json'\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.close()\n",
    "with open(output_file, 'a') as f:\n",
    "    json.dump(total_log, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query에 대해 retrieve된 examples Sampling Experiment\n",
    "\n",
    "sys_msg = \"You are an expert in Dialogue State Tracking(DST) and python coding.\\n\"\n",
    "\n",
    "\n",
    "total_log = []\n",
    "retrieving_samples = True\n",
    "num_retrieved_examples = 100\n",
    "n_smapled_examples = 10\n",
    "sampling_iteration = 10\n",
    "\n",
    "# Randomly select a query data and Retrieve examples from example pool (train data)\n",
    "random.seed(42)\n",
    "query_data = dev_dataset[random.randint(0, len(dev_dataset))]\n",
    "\n",
    "retrieved_examples = embed_query_retrieve_examples(\n",
    "    embedder, example_pool, query_data, emb_keys,\n",
    "    emb_values, label_to_idx, num_retrieved_examples=num_retrieved_examples)\n",
    "\n",
    "example_scores_cnt = {e['ID'] : 0 for e in retrieved_examples}\n",
    "\n",
    "for iteration in range(sampling_iteration):\n",
    "\n",
    "    random.shuffle(retrieved_examples)\n",
    "\n",
    "    sampled_examples = retrieved_examples[:n_smapled_examples]\n",
    "    \n",
    "    msg_chat, gold_python = get_python_chat_prompt(query_data, retrieved_examples, sys_msg)    \n",
    "\n",
    "    log = defaultdict(dict)\n",
    "    examples_list = []\n",
    "    for idx, example in enumerate(sampled_examples):\n",
    "        tmp = {}\n",
    "        tmp['ID_turn-id'] = f\"{example['ID']}-{example['turn_id']}\"\n",
    "        tmp['last_slot_values'], tmp['turn_slot_values'], tmp['slot_values'] = \\\n",
    "            example['last_slot_values'], example['turn_slot_values'], example['slot_values']\n",
    "        examples_list.append(tmp)\n",
    "    log[iteration][\"sampled_retrieve_example\"] = examples_list\n",
    "\n",
    "    log[iteration][\"query\"]['ID-turn-id']= f\"{query_data['ID']}-{query_data['turn_id']}\"\n",
    "    log[iteration][\"query\"]['last_slot_values'] = query_data['last_slot_values'] \n",
    "    log[iteration][\"query\"]['turn_slot_values'] = query_data['turn_slot_values']\n",
    "    log[iteration][\"query\"]['slot_values']= query_data['slot_values']\n",
    "    log[iteration][\"query\"]['gold-python'] = gold_python\n",
    "\n",
    "    args = {\n",
    "        \"model\": 'gpt-3.5-turbo',\n",
    "        \"messages\": msg_chat,\n",
    "        \"max_tokens\": 120,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop\": ['--', '\\n', ';', '#'],\n",
    "        \"n\": 2,\n",
    "        \"logprobs\": True,  # 1 needed to get back log-probabilities at all, in choice['logprobs']['token_logprobs']\n",
    "    }\n",
    "    # results = openai.chat.completions.create(**args)\n",
    "    result_list = []\n",
    "    for i, result in enumerate(results.choices):\n",
    "        tmp = {}\n",
    "        tmp['gold'] = gold_python\n",
    "        tmp['pred'] = result.message.content\n",
    "        tmp['len-normalize-logprob'] = sum(i.logprob for i in result.logprobs.content)/len(result.logprobs.content)\n",
    "        tmp['token_f1'] = calculate_token_f1(encoding, gold_python, tmp['pred'])\n",
    "        # tmp[f'toke-logprobs-{i}'] = [i.logprob for i in result.logprobs.content]\n",
    "        result_list.append(tmp)\n",
    "    sorted_result_list = sorted(result_list, key=lambda x: x['token_f1'], reverse=True)\n",
    "    log[iteration][name] = sorted_result_list\n",
    "    \n",
    "    best_completion = sorted_result_list[0]\n",
    "    predicted_slot_values = parse_python_completion(best_completion['pred'], query_data['last_slot_values'])\n",
    "    aggregate_slot_values = update_dialogue_state(query_data['last_slot_values'], predicted_slot_values) \n",
    "\n",
    "    gold_slot_value = copy.deepcopy(query_data['slot_values'])\n",
    "    for key in gold_slot_value.keys():\n",
    "        if '|' in gold_slot_value[key]:\n",
    "            gold_values = gold_slot_value[key].split('|')\n",
    "            if key in aggregate_slot_values and aggregate_slot_values[key] in gold_values:\n",
    "                gold_slot_value[key] = aggregate_slot_values[key]\n",
    "    jga: int = 1 if aggregate_slot_values == gold_slot_value else 0\n",
    "    f1 = compute_f1(aggregate_slot_values, gold_slot_value)\n",
    "    acc = compute_acc(aggregate_slot_values, gold_slot_value)\n",
    "\n",
    "    log[iteration]['jga'] = jga\n",
    "    log[iteration]['f1'] = f1\n",
    "    log[iteration]['acc'] = acc\n",
    "    \n",
    "    if jga == 1:\n",
    "        for example in sampled_examples:\n",
    "            example_scores_cnt[example['ID']] += 1\n",
    "\n",
    "    total_log.append(log)\n",
    "\n",
    "\n",
    "import json, os\n",
    "output_file = os.getcwd()+f'/sampling_exp.json'\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.close()\n",
    "with open(output_file, 'a') as f:\n",
    "    json.dump(total_log, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.1_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
