{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haesungpyun/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/haesungpyun/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from my_funcs import (\n",
    "    default_transformation, read_json_from_data_dir,\n",
    "    normalize, embed_query_retrieve_examples,\n",
    "    make_two_type_msg, get_python_chat_prompt, \n",
    "    parse_python_completion, update_dialogue_state, \n",
    "    compute_acc, calculate_token_f1, evaluate,\n",
    "    DataOntologyNormalizer, Ontology,\n",
    "    copy, defaultdict, random,\n",
    "    openai, tiktoken, SentenceTransformer\n",
    ")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from my_openai_key import get_openai_key\n",
    "openai.api_key = get_openai_key()\n",
    "\n",
    "from refpydst.utils.general import read_json, read_json_from_data_dir, get_output_dir_full_path\n",
    "from refpydst.retriever.code.mixed_retriever import MixedRetriever\n",
    "from refpydst.retriever.decoders.sampling_topk import SamplingTopK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mw21_5p_train_v1.json', 'r') as f:\n",
    "    train_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../outputs/runs/table4/5p/smapling_exp/split_v1_topk_bm_5_fs_5_0523_0315/running_log.json'\n",
    "with open(path, 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "len(logs)\n",
    "\n",
    "retriever_full_path: str = get_output_dir_full_path(\"../outputs/runs/retriever/mw21_5p_train/referred_states/split_v1/\")\n",
    "retriever = MixedRetriever(**{\n",
    "    \"model_path\": retriever_full_path,\n",
    "    \"search_index_filename\": os.path.join(retriever_full_path, \"train_index.npy\"),\n",
    "    **{\n",
    "        \"datasets\": [train_dataset],\n",
    "        \"sampling_method\": \"pre_assigned\",\n",
    "        **({\"state_transformation\": \"ref_aware\"} or {})\n",
    "    }})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_dict_to_nl(slot_values):\n",
    "    return ', '.join([f\"{k.split('-')[1]} of {k.split('-')[0]} is {v}\" for k, v in slot_values.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ids = [data['ID']+'_turn_'+str(data['turn_id']) for data in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = retriever.data_item_to_embedding(logs[0])\n",
    "bm25_query = retriever.data_item_to_bm25_embedding(logs[0])\n",
    "                \n",
    "sbert_pool=[(turn_label, score) for (turn_label, score) in retriever.retriever.iterate_nearest_dialogs(query, k=len(train_data_ids))]    \n",
    "bm25_pool = [(turn_label, score) for (turn_label, score) in retriever.retriever.bm25_iterate_nearest_dialogs(bm25_query, k=len(train_data_ids))]\n",
    "\n",
    "retrieved_pools = []\n",
    "for idx in range(len(train_data_ids)):\n",
    "    retrieved_pools.append(sbert_pool[idx][0])\n",
    "    if bm25_pool[idx][0] not in retrieved_pools:\n",
    "        retrieved_pools.append(bm25_pool[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2731"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_pool.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1, 2,3, 4, 5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Item: 100%|██████████| 1447/1447 [04:35<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "new_data = defaultdict(list)\n",
    "\n",
    "for data_item in tqdm(logs, desc='Data Item'):\n",
    "    modified_item = copy.deepcopy(data_item)\n",
    "    modified_item['last_slot_values'] = modified_item.get('pred_prior_context', {})\n",
    "    query = retriever.data_item_to_embedding(modified_item)\n",
    "    bm25_query = retriever.data_item_to_bm25_embedding(modified_item)\n",
    "                    \n",
    "    sbert_pool=[(turn_label, score) for (turn_label, score) in retriever.retriever.iterate_nearest_dialogs(query, k=len(train_data_ids))]    \n",
    "    bm25_pool = [(turn_label, score) for (turn_label, score) in retriever.retriever.bm25_iterate_nearest_dialogs(bm25_query, k=len(train_data_ids))]\n",
    "    bm25_pool = list(filter(lambda x: x[0] not in sbert_pool[:50], bm25_pool))\n",
    "    \n",
    "    sorted_train_pool = []\n",
    "    for idx in range(len(train_data_ids)):\n",
    "        if sbert_pool[idx][0] not in sorted_train_pool:\n",
    "            sorted_train_pool.append(sbert_pool[idx][0])\n",
    "        if bm25_pool[idx][0] not in sorted_train_pool:\n",
    "            sorted_train_pool.append(bm25_pool[idx][0])\n",
    "\n",
    "    retrieved_examples_ids = list(modified_item.get('final_scores', {}).get('score_delta', {}).keys())\n",
    "    sorted_train_pool = retrieved_examples_ids + [x for x in sorted_train_pool if x not in retrieved_examples_ids]\n",
    "    \n",
    "    question_list, chosen_list, rejected_list = [], [], []\n",
    "    for iteration in range(1000):\n",
    "        chosen_idx = iteration % 100\n",
    "        chosen_ids = sorted_train_pool[chosen_idx]\n",
    "        if iteration < 300:\n",
    "            rejected_pool = sorted_train_pool[-100:]\n",
    "        elif iteration < 500:\n",
    "            rejected_pool = sorted_train_pool[-len(train_data_ids)//2:-100]\n",
    "        elif iteration < 700:\n",
    "            rejected_pool = sorted_train_pool[200:-len(train_data_ids)//2]\n",
    "        else:\n",
    "            rejected_pool = sorted_train_pool[chosen_idx+1:100+chosen_idx+1]\n",
    "        rejected_ids = random.choice(rejected_pool)\n",
    "\n",
    "        chosen_item = train_dataset[train_data_ids.index(chosen_ids)]\n",
    "        rejected_item = train_dataset[train_data_ids.index(rejected_ids)]\n",
    "\n",
    "        question_str = '[context] ' + str(modified_item['last_slot_values'])\n",
    "        # question_str = '[context] ' + sv_dict_to_nl(modified_item['last_slot_values'])\n",
    "        question_str += ' [utterance_sys] ' + str(modified_item['dialog']['sys'][-1])\n",
    "        question_str += ' [utterance_usr] ' + str(modified_item['dialog']['usr'][-1])\n",
    "        question_list.append(question_str)\n",
    "\n",
    "        chosen_str = '[context] ' + str(chosen_item['last_slot_values'])\n",
    "        # chosen_str = '[context] ' + sv_dict_to_nl(chosen_item['last_slot_values'])\n",
    "        chosen_str += ' [utterance_sys] ' + str(chosen_item['dialog']['sys'][-1])\n",
    "        chosen_str += ' [utterance_usr] ' + str(chosen_item['dialog']['usr'][-1])\n",
    "        chosen_str += ' [belief_state] ' + str(chosen_item['slot_values'])\n",
    "        # chosen_str += '[belief_state] ' + sv_dict_to_nl(chosen_item['slot_values'])\n",
    "        chosen_list.append(chosen_str)\n",
    "        \n",
    "        rejected_str = '[context] ' + str(rejected_item['last_slot_values'])\n",
    "        # rejected_str = '[context] ' + sv_dict_to_nl(rejected_item['last_slot_values'])\n",
    "        rejected_str += ' [utterance_sys] ' + str(rejected_item['dialog']['sys'][-1])\n",
    "        rejected_str += ' [utterance_usr] ' + str(rejected_item['dialog']['usr'][-1])\n",
    "        rejected_str += ' [belief_state] ' + str(rejected_item['slot_values'])\n",
    "        # rejected_str += '[belief_state] ' + sv_dict_to_nl(rejected_item['slot_values'])\n",
    "        rejected_list.append(rejected_str)\n",
    "    new_data['question'].extend(question_list[::1])\n",
    "    new_data['chosen'].extend(chosen_list[::1])\n",
    "    new_data['rejected'].extend(rejected_list[::1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./preference_data.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = defaultdict(list)\n",
    "\n",
    "for data_item in logs:\n",
    "    retrieved_examples_ids = list(logs[0].get('final_scores', {}).get('score_delta', {}).keys())\n",
    "    modified_item = copy.deepcopy(data_item)\n",
    "    modified_item['last_slot_values'] = modified_item.get('pred_prior_context', {})\n",
    "    \n",
    "    if not retrieved_examples_ids:\n",
    "        query = retriever.data_item_to_embedding(modified_item)\n",
    "        bm25_query = retriever.data_item_to_bm25_embedding(modified_item)\n",
    "                        \n",
    "        sbert_pool=[(turn_label, score) for _, (turn_label, score) in zip(range(50), retriever.retriever.iterate_nearest_dialogs(query, k=50))]    \n",
    "        bm25_pool = [(turn_label, score) for (turn_label, score) in retriever.retriever.bm25_iterate_nearest_dialogs(bm25_query, k=100)]\n",
    "\n",
    "        sbert_ids = []\n",
    "        for turn_label, score in sbert_pool:\n",
    "            sbert_ids.append(turn_label)\n",
    "\n",
    "        bm25_pool_filterd = []\n",
    "        for turn_label, score in bm25_pool:\n",
    "            if turn_label not in sbert_ids:\n",
    "                bm25_pool_filterd.append((turn_label, score))\n",
    "        bm25_pool_filterd = bm25_pool_filterd[:50]\n",
    "\n",
    "        for sbert_item, bm25_item in zip(sbert_pool, bm25_pool_filterd):\n",
    "            retrieved_examples_ids.append(sbert_item[0])\n",
    "            retrieved_examples_ids.append(bm25_item[0])\n",
    "    \n",
    "    negative_pool_ids = list(set(train_data_ids) - set(retrieved_examples_ids))\n",
    "    for iteration in range(1000):\n",
    "        chosen_idx = iteration % 100\n",
    "        chosen_ids = retrieved_examples_ids[chosen_idx]\n",
    "        if iteration < 300:\n",
    "            rejected_pool = negative_pool_ids\n",
    "            rejected_ids = random.choice(rejected_pool)\n",
    "        elif iteration < 600:\n",
    "            rejected_pool = retrieved_examples_ids[chosen_idx+1:] + negative_pool_ids\n",
    "            rejected_ids = random.choice(rejected_pool)\n",
    "        else:\n",
    "            rejected_pool =  retrieved_examples_ids[chosen_idx+1:] + negative_pool_ids[:chosen_idx]\n",
    "            rejected_ids = random.choice(rejected_pool)\n",
    "\n",
    "        chosen_item = train_dataset[train_data_ids.index(chosen_ids)]\n",
    "        rejected_item = train_dataset[train_data_ids.index(rejected_ids)]\n",
    "\n",
    "        question_str = '[context] ' + str(modified_item['last_slot_values'])\n",
    "        # question_str = '[context] ' + sv_dict_to_nl(modified_item['last_slot_values'])\n",
    "        question_str += ' [utterance_sys] ' + str(modified_item['dialog']['sys'][-1])\n",
    "        question_str += ' [utterance_usr] ' + str(modified_item['dialog']['usr'][-1])\n",
    "        new_data['question'].append(question_str)\n",
    "\n",
    "        chosen_str = '[context] ' + str(chosen_item['last_slot_values'])\n",
    "        # chosen_str = '[context] ' + sv_dict_to_nl(chosen_item['last_slot_values'])\n",
    "        chosen_str += ' [utterance_sys] ' + str(chosen_item['dialog']['sys'][-1])\n",
    "        chosen_str += ' [utterance_usr] ' + str(chosen_item['dialog']['usr'][-1])\n",
    "        chosen_str += ' [belief_state] ' + str(chosen_item['slot_values'])\n",
    "        # chosen_str += '[belief_state] ' + sv_dict_to_nl(chosen_item['slot_values'])\n",
    "        new_data['chosen'].append(chosen_str)\n",
    "        \n",
    "        rejected_str = '[context] ' + str(rejected_item['last_slot_values'])\n",
    "        # rejected_str = '[context] ' + sv_dict_to_nl(rejected_item['last_slot_values'])\n",
    "        rejected_str += ' [utterance_sys] ' + str(rejected_item['dialog']['sys'][-1])\n",
    "        rejected_str += ' [utterance_usr] ' + str(rejected_item['dialog']['usr'][-1])\n",
    "        rejected_str += ' [belief_state] ' + str(rejected_item['slot_values'])\n",
    "        # rejected_str += '[belief_state] ' + sv_dict_to_nl(rejected_item['slot_values'])\n",
    "        new_data['rejected'].append(rejected_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.1_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
