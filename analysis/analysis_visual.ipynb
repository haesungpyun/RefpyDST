{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표 및 목적\n",
    "- Oracle performance를 찾자!!\n",
    "    - 리트리버 별 무엇을 맞추고 틀렸는지\n",
    "        - 왜 틀린 것인지?\n",
    "        - prediction이 어떻게 다른지, 틀리는 패턴이 있는지?\n",
    "- 리트리버 별 맞춘 인스턴스 overlap 정도 (jaccard score) 확인\n",
    "    - 리트리버에 따라 어떤 test 인스턴스를 잘 맞추는지 확인\n",
    "    - 리트리버를 결합하였을 때, ideal한 성능 예측 확인 가능\n",
    "\n",
    "\n",
    "- 리트리버 별 맞춘 인스턴스에 대한 retrieved examples (주로 10, 16) overlap 정도 (jaccard score) 확인\n",
    "    - (비슷한 풀이로 문제를 맞췄다) 겹치는 examples 많은 경우, 해당 example들이 정답을 맞추는데 큰 도움이 될 것\n",
    "        - 겹치는 examples만 inference 해보기 -> 강력하게 정답 맞추게 돕는 set 특성 파악\n",
    "            - Semantic n Lexical 한 경우면 더욱 적은 shot으로도 맞출 수 있지 않을까?\n",
    "                - 비슷한 성능이라면 Dense retriever -> fine-tuned SBERT / pretrained SBERT / Other Encoder / GPT2 변경해볼 수 있지 않을까? (GPT friendly)\n",
    "    - (다른 풀이로 문제를 맞췄다) 겹치는 examples 적은 경우, 두 examples set들에 대한 추가 분석 필요 -> 어떤 면이 다르고 같은지\n",
    "        - (교수님의 8개 씩 sampling+scoring) 두 set을 합치고 sampling 해 어떤 sample이 빠질 때, 혹은 어떤 sample이 추가될 때 틀리거나 맞는지 확인\n",
    "            - 두 set 중에서 어떤 sample이나 조합이 강력하게 정답을 맞추게 돕는지 특성 파악\n",
    "                - 어떻게 해당 특성의 examples만 고를 수 있을까?\n",
    "    \n",
    "- 리트리버 별 틀린 인스턴스에 대한 retrieved examples (주로 10, 16) overlap 정도 (jaccard score) 확인\n",
    "    - (비슷한 풀이로 문제를 틀렸다) 겹치는 examples 많은 경우, 해당 example들이 정답을 맞추는데 방해가 되는 example?\n",
    "        - 겹치는 examples 빼고 inference 해보기 -> distact example set 특성 파악\n",
    "            - 어떻게 해당 특성의 example을 필터링할 수 있을까? \n",
    "    - (다른 풀이로 문제를 틀렸다) 겹치는 examples 적은 경우, 두 examples set들에 대한 추가 분석 필요 -> 어떤 면이 다르고 같은지\n",
    "        - (교수님의 8개 씩 sampling+scoring) 두 set을 합치고 sampling해 어떤 sample이 빠질 때, 혹은 어떤 sample이 추가될 때 틀리거나 맞는지 확인\n",
    "            - 맞추는 경우 -> 빠진 sample이나 조합이 disctract, 틀리는 경우 -> 두 set 모두 정답을 support x\n",
    "\n",
    "- 하나의 리트리버에서 맞춘 인스턴스의 retrieved examples와 다른 리트리버에서 틀린 인스턴스의 retrieved examples의 overlap 정도 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp config\n",
    "fine-tune dataset: mw21_5p_train_v1    \n",
    "evaluate dataset: mw24_20p_dev\n",
    "     \n",
    "- \"retriever_type\": \"bm25\" / \"fine_tuned_sbert\" / \"pretrained_sbert\",     \n",
    "- \"# of demos\": 8 / 10 / 16 / 32, &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; # number of demos when constructing the prompt        \n",
    "- \"decoding pool\": \"ret(retreived pool 100 samples)\" / \"all(train set)\", &emsp;&emsp;&emsp;&emsp; # For BM24 when calculate TF-IDF, using different pool may have different results     \n",
    "- \"decoding strategy\": \"sim(similarity)\" / \"sim_div(simiarity and diversity)\" &emsp;&emsp;&emsp;&emsp; # Decoding strategy. sim_div is used in the paper     \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyun/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/pyun/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 24af7adf9c18fc8ebd8af481021a6dcaa10fb2cf
   "source": [
    "import os\n",
    "import json \n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from refpydst.evaluate_metrics import evaluate\n",
    "from refpydst.utils.dialogue_state import update_dialogue_state\n",
    "from refpydst.prompt_formats.python.completion_parser import parse_python_completion, iterative_parsing, parse_python_modified\n",
    "from refpydst.normalization.data_ontology_normalizer import DataOntologyNormalizer\n",
    "from refpydst.db.ontology import Ontology\n",
    "import refpydst.prompt_formats.python.demo as python_demo\n",
    "from refpydst.prompt_formats.python.completion_parser import *\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7368"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/mw24_100p_test.json', 'r') as f:\n",
    "    test = json.load(f)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mapping supervised_set surface forms...: 100%|██████████| 2731/2731 [00:04<00:00, 623.74it/s] \n",
      "reading surface forms from ontology.json: 100%|██████████| 31/31 [00:02<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/mw21_5p_train_v1.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "normalizer = DataOntologyNormalizer(\n",
    "        Ontology.create_ontology(),\n",
    "        # count labels from the train set\n",
    "        supervised_set=train_data,\n",
    "        # make use of existing surface form knowledge encoded in ontology.json, released with each dataset\n",
    "        # see README.json within https://github.com/smartyfh/MultiWOZ2.4/raw/main/data/MULTIWOZ2.4.zip\n",
    "        counts_from_ontology_file=\"../src/refpydst/db/multiwoz/2.4/ontology.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parsing(python_completion: str, state: Union[MultiWOZDict, ParserBeliefState] = None,\n",
    "                            exceptions_are_empty: bool = True, **kwargs) -> MultiWOZDict:\n",
    "    if not type(state) == ParserBeliefState:\n",
    "        state = parser_belief_state_from_mwoz_dict(state)\n",
    "\n",
    "    full_statement = \"agent.state.\" + python_completion.strip()\n",
    "    full_statement = replace_state_references(full_statement)\n",
    "    \n",
    "    agent = ParserAgent()\n",
    "    agent.state = ParserBeliefState()\n",
    "\n",
    "    for statement in full_statement.splitlines():\n",
    "        if not statement.startswith(\"agent.state.\"):\n",
    "            statement = \"agent.state.\" + statement.strip()\n",
    "       \n",
    "        if statement.strip().startswith(\"agent.state.del \"):\n",
    "            try:\n",
    "                statement = statement.replace(\"agent.state.del \", \"\").replace('agent.state.', '')\n",
    "                domain, slot = statement.split('.')[-2], statement.split('.')[-1]\n",
    "                statement = statement.replace(f\"{domain}.{slot}\", '')\n",
    "                exec(f'agent.state.{domain} = agent.find_{domain}({slot}= \"[DELETE]\")')\n",
    "            except Exception as e:\n",
    "                print(pprint.pformat(e))\n",
    "                print(f\"got exception when deal with del: {statement}\\n\")\n",
    "                continue\n",
    "            continue\n",
    "        \n",
    "        prefix = statement.split('(')[0] \n",
    "        try:                        \n",
    "            args_list = statement.split('(')[1].split(',')\n",
    "            args_list[-1] = args_list[-1].replace(')', '') if args_list[-1].endswith(')') else args_list[-1]\n",
    "        except Exception as e:\n",
    "            print('----------------- MY PARSING ERROR -----------------')\n",
    "            print(pprint.pformat(e))\n",
    "            print(f\"got exception when splitting statement: {statement}\")\n",
    "            print(f\"current state: {sorted_dict(agent.state.to_mwoz_dict())}\\n\")\n",
    "\n",
    "            continue\n",
    "        \n",
    "        new_arg_list = []\n",
    "        for arg in args_list:\n",
    "            if 'agent.state.' in arg:\n",
    "                arg = arg.replace('agent.state.', 'state.')\n",
    "            new_arg_list.append(arg)\n",
    "        try:\n",
    "            sub_statement = prefix + '(' + ','.join(new_arg_list) + ')'\n",
    "            exec(sub_statement)\n",
    "        except Exception as e:\n",
    "            print('----------------- MY PARSING ERROR -----------------')\n",
    "            print(pprint.pformat(e))\n",
    "            print(f\"got exception when execute statement: {sub_statement}\\n previous state: {sorted_dict(state.to_mwoz_dict())}\")\n",
    "            print(f\"current state: {sorted_dict(agent.state.to_mwoz_dict())}\\n\")\n",
    "            continue\n",
    "\n",
    "    return agent.state.to_mwoz_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_parsing(python_completion: str, state: Union[MultiWOZDict, ParserBeliefState] = None,\n",
    "                            exceptions_are_empty: bool = True, **kwargs) -> MultiWOZDict:\n",
    "    if not type(state) == ParserBeliefState:\n",
    "        state = parser_belief_state_from_mwoz_dict(state)\n",
    "\n",
    "    full_statement = \"agent.state.\" + python_completion.strip()\n",
    "    full_statement = replace_state_references(full_statement)\n",
    "    \n",
    "    agent = ParserAgent()\n",
    "    agent.state = ParserBeliefState()\n",
    "\n",
    "    for statement in full_statement.splitlines():\n",
    "        if not statement.startswith(\"agent.state.\"):\n",
    "            statement = \"agent.state.\" + statement.strip()\n",
    "        \n",
    "        if statement.strip().startswith(\"agent.state.del \"):\n",
    "            try:\n",
    "                statement = statement.replace(\"agent.state.del \", \"\").replace('agent.state.', '')\n",
    "                domain, slot = statement.split('.')[-2], statement.split('.')[-1]\n",
    "                statement = statement.replace(f\"{domain}.{slot}\", '')\n",
    "                exec(f'agent.state.{domain} = agent.find_{domain}({slot}= \"[DELETE]\")')\n",
    "            except Exception as e:\n",
    "                print(pprint.pformat(e))\n",
    "                print(f\"got exception when deal with del: {statement}\\n\")\n",
    "                print(f\"current state: {sorted_dict(agent.state.to_mwoz_dict())}\\n\")\n",
    "                continue\n",
    "            continue\n",
    "\n",
    "        prefix = statement.split('(')[0] \n",
    "        try:                        \n",
    "            args_list = statement.split('(')[1].split(',')\n",
    "            args_list[-1] = args_list[-1].replace(')', '') if args_list[-1].endswith(')') else args_list[-1]\n",
    "        except Exception as e:\n",
    "            print('----------------- ITERATIVE PARSING ERROR -----------------')\n",
    "            print(pprint.pformat(e))\n",
    "            print(f\"got exception when splitting statement: {statement}\\n\")\n",
    "            continue\n",
    "        \n",
    "        for arg in args_list:\n",
    "            if 'agent.state.' in arg:\n",
    "                arg = arg.replace('agent.state.', 'state.')\n",
    "            try:\n",
    "                sub_statement = prefix + '(' + arg + ')'\n",
    "                exec(sub_statement)\n",
    "            except Exception as e:\n",
    "                print('----------------- ITERATIVE PARSING ERROR -----------------')\n",
    "                print(pprint.pformat(e))\n",
    "                print(f\"got exception when execute statement: {sub_statement}\\n previous state: {sorted_dict(state.to_mwoz_dict())}\")\n",
    "                print(f\"current state: {sorted_dict(agent.state.to_mwoz_dict())}\\n\")\n",
    "                continue\n",
    "                \n",
    "    return agent.state.to_mwoz_dict()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'jga'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2061005/534557461.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# experiments.append({'name': exp_name, 'right': right, 'wrong': wrong,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m#                            'jga': jga, 'slot_acc': slot_acc, 'slot_f1': slot_f1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#                            'shots':right_shots+wrong_shots, 'right_shots':right_shots, 'wrong_shots':wrong_shots,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#                            'right_logs':right_logs, 'wrong_logs':wrong_logs})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jga'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;31m# experiments = sorted(experiments, key=lambda x: x['jga'], reverse=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# num_exps = len(experiments)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../stats_og_parsing_modified.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6318\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6319\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6321\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6322\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6324\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch2.1_clone/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1836\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'jga'"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 24af7adf9c18fc8ebd8af481021a6dcaa10fb2cf
   "source": [
    "stats = pd.DataFrame()\n",
    "experiments = []\n",
    "\n",
    "for path, dir, files in os.walk('/home/haesungpyun/my_refpydst/outputs/runs/table4/'):\n",
    "    if 'running_log.json' in files:\n",
    "        exp_name = path.split('/split_v1')[-1]\n",
    "\n",
    "        exp_name = path.split('/')[-2] + exp_name\n",
    "        log_path = os.path.join(path, 'running_log.json')\n",
    "        # if \"topk_bm_5_fs_5_0523_0315\" not in exp_name:\n",
    "        #     continue\n",
    "        with open(log_path, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "        \n",
    "        jga_by_turn_id = defaultdict(list)  # use to record the accuracy\n",
    "        jga_by_dialog = defaultdict(list)  # use to record the accuracy\n",
    "        \n",
    "        total_acc, total_f1 = 0, 0\n",
    "        n_correct = 0\n",
    "        n_total = len(logs)\n",
    "        \n",
    "        right, right_shots, right_logs = [], [], []\n",
    "        wrong, wrong_shots, wrong_logs = [], [], []\n",
    "        \n",
    "        prior_pred, prior_id = None, None\n",
    "        for data_item in logs:\n",
    "            # pred = data_item['pred']\n",
    "            if data_item.get('completion') is None:\n",
    "                n_correct += 1\n",
    "                tmp = []\n",
    "                for ex in data_item.get('examples', []):\n",
    "                    tmp.append(ex[0].replace('.json', '_')+str(ex[1]))\n",
    "                data_item.update({'examples':tmp})\n",
    "                right_shots.append({\n",
    "                    data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):tmp})\n",
    "                right_logs.append({data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):data_item})\n",
    "                right.append(data_item['ID'].replace('.json', '_')+str(data_item['turn_id']))\n",
    "\n",
    "                prior_id = data_item['ID']\n",
    "                prior_pred = data_item['pred']\n",
    "                # prior_pred_2 = data_item['pred']\n",
    "                continue\n",
    "            \n",
    "            if data_item['ID'] != prior_id:\n",
    "                prior_pred = data_item['pred_prior_context']\n",
    "                # prior_pred_2 = data_item['pred_prior_context']\n",
    "            pred_delta = normalizer.normalize(my_parsing(data_item['completion'], prior_pred))\n",
    "            # pred_delta_2 =  normalizer.normalize(iterative_parsing(data_item['completion'], prior_pred))\n",
    "            pred =  update_dialogue_state(prior_pred, pred_delta)\n",
    "            # pred_2 = update_dialogue_state(prior_pred_2, pred_delta_2)\n",
    "\n",
    "            # pred = data_item['pred']\n",
    "\n",
    "            this_jga, this_acc, this_f1 = evaluate(pred, data_item['slot_values'])\n",
    "            total_acc += this_acc\n",
    "            total_f1 += this_f1\n",
    "\n",
    "            if this_jga:\n",
    "                n_correct += 1\n",
    "                tmp = []\n",
    "                # for ex in data_item.get('examples', []):\n",
    "                #     tmp.append(ex[0].replace('.json', '_')+str(ex[1]))\n",
    "                # data_item.update({'examples':tmp})\n",
    "                # right_shots.append({\n",
    "                #     data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):tmp})\n",
    "                # right_logs.append({data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):data_item})\n",
    "                # right.append(data_item['ID'].replace('.json', '_')+str(data_item['turn_id']))\n",
    "            else:\n",
    "                tmp = []\n",
    "                # for ex in data_item['examples']:\n",
    "                #     tmp.append(ex[0].replace('.json', '_')+str(ex[1]))\n",
    "                # data_item.update({'examples':tmp})\n",
    "                # wrong_shots.append({\n",
    "                #     data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):tmp})\n",
    "                # wrong_logs.append({data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):data_item})\n",
    "                # wrong.append(data_item['ID'].replace('.json', '_')+str(data_item['turn_id']))\n",
    "\n",
    "            prior_id = data_item['ID']\n",
    "            prior_pred = pred\n",
    "\n",
    "        jga = n_correct / n_total\n",
    "        slot_acc = total_acc/n_total\n",
    "        slot_f1 = total_f1/n_total\n",
    "\n",
    "        stats = pd.concat([\n",
    "            stats, pd.DataFrame({\n",
    "                'name': [exp_name], 'jga': [jga], 'right': n_correct, 'wrong': n_total-n_correct, \n",
    "                'slot_acc': [slot_acc], 'slot_f1': [slot_f1]})])\n",
    "        # experiments.append({'name': exp_name, 'right': right, 'wrong': wrong, \n",
    "        #                            'jga': jga, 'slot_acc': slot_acc, 'slot_f1': slot_f1, \n",
    "        #                            'shots':right_shots+wrong_shots, 'right_shots':right_shots, 'wrong_shots':wrong_shots, \n",
    "        #                            'right_logs':right_logs, 'wrong_logs':wrong_logs})\n",
    "stats = stats.sort_values(by='jga', ascending=False).reset_index(drop=True)\n",
    "# experiments = sorted(experiments, key=lambda x: x['jga'], reverse=True)\n",
    "# num_exps = len(experiments)\n",
    "stats.to_csv('../stats_og_parsing_modified.csv', index=False, sep='\\t')\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv('../stats_llama_og_parsing_modified.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_dict(dict_a, by_key=True):\n",
    "    if by_key:\n",
    "        return dict(sorted(dict_a.items(), key=lambda item: item[0]))\n",
    "    else:\n",
    "        return dict(sorted(dict_a.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder_path = '/home/haesungpyun/my_refpydst/outputs/runs/'\n",
    "experiment_folder_path += 'table4_llama/'\n",
    "def collect_stats(experiment_folder_path):\n",
    "    stats = pd.DataFrame()\n",
    "    experiments = []\n",
    "    for path, dir, files in os.walk(experiment_folder_path):\n",
    "        if 'running_log.json' in files:\n",
    "            exp_name = path.split('/split_v1')[-1]\n",
    "\n",
    "            exp_name = path.split('/')[-2] + exp_name\n",
    "            log_path = os.path.join(path, 'running_log.json')\n",
    "            # if \"topk_bm_5_fs_5_0523_0315\" not in exp_name:\n",
    "            #     continue\n",
    "            with open(log_path, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "            \n",
    "            jga_by_turn_id = defaultdict(list)  # use to record the accuracy\n",
    "            jga_by_dialog = defaultdict(list)  # use to record the accuracy\n",
    "            \n",
    "            total_acc, total_f1 = 0, 0\n",
    "            n_correct = 0\n",
    "            n_total = len(logs)\n",
    "            \n",
    "            right, right_shots, right_logs = [], [], []\n",
    "            wrong, wrong_shots, wrong_logs = [], [], []\n",
    "            \n",
    "            prior_pred, prior_id = None, None\n",
    "            for data_item in logs:\n",
    "                # pred = data_item['pred']\n",
    "                if data_item.get('completion') is None:\n",
    "                    n_correct += 1\n",
    "                    tmp = []\n",
    "                    for ex in data_item.get('examples', []):\n",
    "                        tmp.append(ex[0].replace('.json', '_')+str(ex[1]))\n",
    "                    data_item.update({'examples':tmp})\n",
    "                    right_shots.append({\n",
    "                        data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):tmp})\n",
    "                    right_logs.append({data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):data_item})\n",
    "                    right.append(data_item['ID'].replace('.json', '_')+str(data_item['turn_id']))\n",
    "\n",
    "                    prior_id = data_item['ID']\n",
    "                    prior_pred = data_item['pred']\n",
    "                    # prior_pred_2 = data_item['pred']\n",
    "                    continue\n",
    "                \n",
    "                if data_item['ID'] != prior_id:\n",
    "                    prior_pred = data_item['pred_prior_context']\n",
    "                    # prior_pred_2 = data_item['pred_prior_context']\n",
    "                pred_delta = normalizer.normalize(my_parsing(data_item['completion'], prior_pred))\n",
    "                # pred_delta_2 =  normalizer.normalize(iterative_parsing(data_item['completion'], prior_pred))\n",
    "                pred =  update_dialogue_state(prior_pred, pred_delta)\n",
    "                # pred_2 = update_dialogue_state(prior_pred_2, pred_delta_2)\n",
    "\n",
    "                # pred = data_item['pred']\n",
    "\n",
    "                this_jga, this_acc, this_f1 = evaluate(pred, data_item['slot_values'])\n",
    "                total_acc += this_acc\n",
    "                total_f1 += this_f1\n",
    "\n",
    "                if this_jga:\n",
    "                    n_correct += 1\n",
    "                    tmp = []\n",
    "                   \n",
    "                else:\n",
    "                    tmp = []\n",
    "                    # for ex in data_item['examples']:\n",
    "                    #     tmp.append(ex[0].replace('.json', '_')+str(ex[1]))\n",
    "                    # data_item.update({'examples':tmp})\n",
    "                    # wrong_shots.append({\n",
    "                    #     data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):tmp})\n",
    "                    # wrong_logs.append({data_item['ID'].replace('.json', '_')+str(data_item['turn_id']):data_item})\n",
    "                    # wrong.append(data_item['ID'].replace('.json', '_')+str(data_item['turn_id']))\n",
    "\n",
    "                prior_id = data_item['ID']\n",
    "                prior_pred = pred\n",
    "\n",
    "            jga = n_correct / n_total\n",
    "            slot_acc = total_acc/n_total\n",
    "            slot_f1 = total_f1/n_total\n",
    "\n",
    "            stats = pd.concat([\n",
    "                stats, pd.DataFrame({\n",
    "                    'name': [exp_name], 'jga': [jga], 'right': n_correct, 'wrong': n_total-n_correct, \n",
    "                    'slot_acc': [slot_acc], 'slot_f1': [slot_f1]})])\n",
    "            # experiments.append({'name': exp_name, 'right': right, 'wrong': wrong, \n",
    "            #                            'jga': jga, 'slot_acc': slot_acc, 'slot_f1': slot_f1, \n",
    "            #                            'shots':right_shots+wrong_shots, 'right_shots':right_shots, 'wrong_shots':wrong_shots, \n",
    "            #                            'right_logs':right_logs, 'wrong_logs':wrong_logs})\n",
    "    stats = stats.sort_values(by='jga', ascending=False).reset_index(drop=True)\n",
    "    return stats\n",
    "# experiments = sorted(experiments, key=lambda x: x['jga'], reverse=True)\n",
    "# num_exps = len(experiments)\n",
    "# stats.to_csv('../stats_og_parsing_modified.csv', index=False, sep='\\t')\n",
    "# stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = collect_stats(experiment_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv('../stats_llama_og_parsing_modified.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity\n",
    "def jaccard(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    return len(a.intersection(b)) / len(a.union(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test instance 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower traingle is jaccard score about right examples, Upper is jaccard score about wrong examples\n",
    "# The last column is jga score of each retrieval method\n",
    "zero_right = set(experiments[-1]['right'])\n",
    "zero_wrong = set(experiments[-1]['wrong'])\n",
    "\n",
    "total_len = len(experiments[0]['shots'])\n",
    "\n",
    "right_overlap = defaultdict(dict)\n",
    "wrong_overlap = defaultdict(dict)\n",
    "no_zero_overlap = defaultdict(dict)\n",
    "for idx1 in range(len(experiments)):\n",
    "    for idx2 in range(idx1, len(experiments)):    \n",
    "        exp_name_1, exp_name_2  = experiments[idx1]['name'], experiments[idx2]['name']\n",
    "\n",
    "        A_right, A_wrong = set(experiments[idx1]['right']), set(experiments[idx1]['wrong'])\n",
    "        B_right, B_wrong = set(experiments[idx2]['right']), set(experiments[idx2]['wrong'])\n",
    "        A_r_len, A_w_len = len(experiments[idx1]['right']), len(experiments[idx1]['wrong'])\n",
    "        B_r_len, B_w_len = len(experiments[idx2]['right']), len(experiments[idx2]['wrong'])\n",
    "        \n",
    "        right_overlap[exp_name_1][exp_name_2] = len(A_right.intersection(B_right)) #/ total_len #A_r_len\n",
    "        wrong_overlap[exp_name_1][exp_name_2] = len(A_wrong.intersection(B_wrong)) #/ total_len #A_w_len\n",
    "        no_zero_overlap[exp_name_1][exp_name_2] = len((A_right - zero_right).intersection(B_right - zero_right)) #/total_len # A_r_len\n",
    "\n",
    "        if exp_name_1 != exp_name_2:\n",
    "            right_overlap[exp_name_2][exp_name_1] = len(A_right.intersection(B_right)) #/ total_len #B_r_len\n",
    "            wrong_overlap[exp_name_2][exp_name_1] = len(A_wrong.intersection(B_wrong)) #/ total_len #B_w_len\n",
    "            no_zero_overlap[exp_name_2][exp_name_1] = len((A_right - zero_right).intersection(B_right - zero_right)) #/ total_len #B_r_len\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(27, 9))\n",
    "sns.heatmap(pd.DataFrame(right_overlap), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, ax=axes[0]).set_title('Right Examples Overlap')\n",
    "sns.heatmap(pd.DataFrame(wrong_overlap), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, ax=axes[1], yticklabels=False).set_title('Wrong Examples Overlap')\n",
    "sns.heatmap(pd.DataFrame(no_zero_overlap), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, ax=axes[2], yticklabels=False).set_title('Rigt Examples Overlap except Zero shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 같은 encoder 끼리는 매우 높은 유사도를 갖음\n",
    "# - fine_tuned_sbert - bm25_10_all_sim도 매우 높은 유사도\n",
    "#     - 맞춘 정답이 많아서 많이 겹치는 듯\n",
    "# - fine_tuned_sbert - pretrained_sbert는 상대적으로 낮은 유사도\n",
    "# - bm25 - pretrained_sbert는 유사도가 높은 편\n",
    "# - fine_tuned_sbert <-> bm25, pretrained_sbert\n",
    "#     - bm25와 pretrained_sbert는 비슷한 문제를 맞추는 듯 함\n",
    "#     - fine_tuned_sbert와는 다른 양상인 듯\n",
    "# - zero_shot vs Other methods\n",
    "#     - 각 method가 맞춘 instance의 약 50% \n",
    "#         - 비슷한 수(360-380)의 instance가 겹치지만, 각 method 별 맞춘 개수가 달라, 성능이 낮을 수록 겹치는 정도가 올라감\n",
    "#     - zero_shot의 맞춘 instance가 각 method가 맞춘 instance 와 겹치는 정도는 약 85-90%\n",
    "#         - method의 성능이 늘어날 수록 겹치는 정도가 증가함\n",
    "#         - 높은 성능의 method는 맞춘 instance가 많고, 그래서 더욱 많은 instance를 커버하기 때문\n",
    "\n",
    "# TODO\n",
    "# - zero_shot 이외 다른 50%(어려운 instance)에 대해, 겹치는 정도와 분석이 필요함\n",
    "# - zero_shot 중 다른 method가 맞추지 못한 15-10%의 instance에 대한 분석 필요\n",
    "\n",
    "# - retrieval method는 같은데, decoding 방식이 다른 것들을 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Retrieved examples set 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jaccard_overlap(experiments, shot_type_1, shot_type_2):\n",
    "    jaccard_map = defaultdict(dict)\n",
    "    overlap_map = defaultdict(dict)\n",
    "\n",
    "    for idx1 in range(len(experiments)):\n",
    "        exp_name_1 = experiments[idx1]['name']\n",
    "        for idx2 in range(len(experiments)):\n",
    "            exp_name_2 = experiments[idx2]['name']\n",
    "\n",
    "            A_q_e_list = sorted(experiments[idx1][shot_type_1], key=lambda x: list(x.keys())[0])\n",
    "            B_q_e_list = sorted(experiments[idx2][shot_type_2], key=lambda x: list(x.keys())[0])\n",
    "            \n",
    "            overlap_map[exp_name_1][exp_name_2] = []\n",
    "            jaccard_score = 0\n",
    "            for q_e_list in A_q_e_list:\n",
    "                q = list(q_e_list.keys())[0]\n",
    "                A = q_e_list[q]\n",
    "                a_in_B = list(filter(lambda x: list(x.keys())[0] == q, B_q_e_list))\n",
    "                # For right query in exp_name_1, find the same query in exp_name_2 right queries.\n",
    "                # If there is a same query, calculate the jaccard score.\n",
    "                if len(a_in_B) > 0:\n",
    "                    B = list(filter(lambda x: list(x.keys())[0] == q, B_q_e_list))[0][q]\n",
    "                    jaccard_score += jaccard(A, B)\n",
    "                    A_B = len(set(A).intersection(set(B)))\n",
    "                    A_Bc = len(set(A)-set(B))\n",
    "                    Ac_B = len(set(B)-set(A))\n",
    "                    overlap_map[exp_name_1][exp_name_2].append([A_B, A_Bc, Ac_B])\n",
    "                \n",
    "            jaccard_map[exp_name_1][exp_name_2] = jaccard_score/len(A_q_e_list) if len(A_q_e_list) > 0 else jaccard_score\n",
    "\n",
    "    num_overlaps = copy.deepcopy(jaccard_map)\n",
    "    for k1 in jaccard_map:\n",
    "        for k2 in jaccard_map[k1]:\n",
    "            if k2 == k1:\n",
    "                jaccard_map[k1][k1] = 0\n",
    "                num_overlaps[k1][k1] = 0\n",
    "            num_overlaps[k1][k2] = (jaccard_map[k1][k2]*20) / (1 + jaccard_map[k1][k2])\n",
    "            num_overlaps[k1][k2] = round(num_overlaps[k1][k2], 4)\n",
    "    return jaccard_map, num_overlaps, overlap_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_jaccard_map, shot_num_overlaps, _ = make_jaccard_overlap(experiments, 'shots', 'shots')\n",
    "right_jaccard_map, right_num_overlaps, _ = make_jaccard_overlap(experiments, 'right_shots', 'right_shots')\n",
    "wrong_jaccard_map, wrong_num_overlaps, _ = make_jaccard_overlap(experiments, 'wrong_shots', 'wrong_shots')\n",
    "right_wrong_jaccard_map, right_wrong_num_overlaps, _ = make_jaccard_overlap(experiments, 'right_shots', 'wrong_shots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,4, figsize=(28, 14))\n",
    "sns.heatmap(pd.DataFrame(shot_jaccard_map), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, xticklabels=False, ax=axes[0][0]).set_title('All shot Jaccard Map')\n",
    "sns.heatmap(pd.DataFrame(shot_num_overlaps), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, xticklabels=False, yticklabels=False, ax=axes[0][1]).set_title('All shot # of Overlaps')\n",
    "sns.heatmap(pd.DataFrame(right_jaccard_map), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, xticklabels=False, yticklabels=False, ax=axes[0][2]).set_title('Right Shot Jaccard Map')\n",
    "sns.heatmap(pd.DataFrame(right_num_overlaps), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, xticklabels=False, yticklabels=False, ax=axes[0][3]).set_title('Right shot # of Overlaps')\n",
    "sns.heatmap(pd.DataFrame(wrong_jaccard_map), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, ax=axes[1][0]).set_title('Wrong Shot Jaccard Map')\n",
    "sns.heatmap(pd.DataFrame(wrong_num_overlaps), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, yticklabels=False, ax=axes[1][1]).set_title('Wrong shot # of Overlaps')\n",
    "sns.heatmap(pd.DataFrame(right_wrong_jaccard_map), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, yticklabels=False, ax=axes[1][2]).set_title('Right and Wrong Shot Jaccard Map')\n",
    "sns.heatmap(pd.DataFrame(right_wrong_num_overlaps), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, yticklabels=False, ax=axes[1][3]) .set_title('Righ and Wrong shot # of Overlaps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_right = set(experiments[-1]['right'])\n",
    "for exp in experiments:\n",
    "    exp['right_except_zero'] = []\n",
    "    exp['right_and_zero'] = []\n",
    "    for ids in set(exp['right']).intersection(zero_right):\n",
    "        right_shots = list(filter(lambda x: list(x.keys())[0] == ids, exp['right_shots']))[0]\n",
    "        exp['right_and_zero'].append(right_shots)\n",
    "    for ids in set(exp['right']) - zero_right:\n",
    "        right_shots = list(filter(lambda x: list(x.keys())[0] == ids, exp['right_shots']))[0]\n",
    "        exp['right_except_zero'].append(right_shots)\n",
    "\n",
    "righ_and_zero_jaccard_map,righ_and_zero_num_overlaps, _ = make_jaccard_overlap(experiments, 'right_and_zero', 'right_and_zero')\n",
    "right_except_zero_jaccard_map, right_except_zero_num_overlaps, _ = make_jaccard_overlap(experiments, 'right_except_zero', 'right_except_zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4, figsize=(28 ,7))\n",
    "sns.heatmap(pd.DataFrame(righ_and_zero_jaccard_map), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, ax=axes[0]).set_title('Right shot n Zero shot    Jaccard Map')\n",
    "sns.heatmap(pd.DataFrame(righ_and_zero_num_overlaps), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, yticklabels=False, ax=axes[1]).set_title('Right shot n Zero shot    # of Overlaps')\n",
    "sns.heatmap(pd.DataFrame(right_except_zero_jaccard_map), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, yticklabels=False, ax=axes[2]).set_title('Right Shot except Zero shot right    Jaccard Map')\n",
    "sns.heatmap(pd.DataFrame(right_except_zero_num_overlaps), annot=True, cmap='coolwarm', fmt=\".4f\", cbar=False, yticklabels=False, ax=axes[3]).set_title('Right shot except Zero shot right    # of Overlaps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > len(AnB) / len(AuB) = len(AnB) / (len(A) + len(B) - len(AnB))   \n",
    "# > #of overlap examples = jaccard_score*(len(A)+len(B)) / (1+jaccard_score)\n",
    "\n",
    "# 1. fine_tuned_sbert vs fine_tuned_sbert_topk -> # of overlap examples = 4\n",
    "# 2. fine_tuned_sbert vs bm25 -> # of overlap examples = 1.1\n",
    "# 3. fine_tuned_sbert vs pretrained_sbert -> # of overlap examples = 0.88\n",
    "# 4. bm25_10_all_sim vs bm25_10_all_sim_div -> # of overlap examples = 4.5\n",
    "# 5. bm25_10_all_sim vs pretrained_sbert -> # of overlap examples = 2\n",
    "# 6. pretrained_sbert vs pretrained_sbert_topk => # of examples =3.79\n",
    "\n",
    "# 같은 retriever, 다른 decoding -> 10개, 10개 총 20개의 retrieved examples 중 4개만 겹침 -> diversity로 6개의 새로운 examples 추가됨          \n",
    "# pretrained_sbert와 bm25는 fine_tuned_sbert보다 서로 어느 정도 유사한 example들을 뽑아 오는 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------EOC----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_python_chat_prompt(data_item, examples, n_examples: int = None,\n",
    "                          reverse_x_and_y=False, use_null_data_item= False,\n",
    "                          detailed_state_string: bool = True) -> str:\n",
    "    \n",
    "    msg = [{\"role\": \"system\", \"content\": \"You are an expert in Dialogue State Tracking(DST) and python coding.\\n\"}]\n",
    "    given_context = data_item['last_slot_values']\n",
    "    max_n_examples: int = n_examples is not None and n_examples or len(examples)\n",
    "\n",
    "    # in case for zero-shot learning\n",
    "    if max_n_examples > 0:\n",
    "        for example_id, example in enumerate(examples[-max_n_examples:]):\n",
    "            user_msg = f\"\\n    #### Example {example_id + 1} ####\\n\"\n",
    "            assist_chat_msg = \"\"\n",
    "            # remove multiple choice in last slot values\n",
    "            last_slot_values = {s: v.split('|')[0] for s, v in example['last_slot_values'].items()}\n",
    "\n",
    "            last_sys_utt = example['dialog']['sys'][-1]\n",
    "            if last_sys_utt == 'none':\n",
    "                last_sys_utt = ''\n",
    "            user_string = python_demo.get_user_string(example['dialog']['usr'][-1])\n",
    "            state_string, update_string = python_demo.get_python_statements(last_slot_values, example['slot_values'],\n",
    "                                                                            turn_strings=[last_sys_utt, user_string],\n",
    "                                                                            detailed_state_string=detailed_state_string)\n",
    "            user_msg += \"    \" + state_string + \"\\n\"\n",
    "            if last_sys_utt:\n",
    "                user_msg += \"    \" + python_demo.get_system_string(last_sys_utt) + \"\\n\"\n",
    "            user_msg += \"    \" + user_string + \"\\n\"\n",
    "            \n",
    "            for s in update_string.split(\"\\n\"):\n",
    "                assist_chat_msg += \"    \" + s.strip() + \"\\n\"\n",
    "            if not reverse_x_and_y:\n",
    "                msg.append({\"role\": \"user\",\"content\": user_msg})\n",
    "                msg.append({\"role\": \"assistant\",\"content\": assist_chat_msg})\n",
    "            else:\n",
    "                msg.append({\"role\": \"assistant\",\"content\": assist_chat_msg})\n",
    "                msg.append({\"role\": \"user\",\"content\": user_msg})\n",
    "\n",
    "    user_msg = f\"\\n    #### Example {max_n_examples + 1} ####\\n\"\n",
    "    if given_context is None:\n",
    "        last_slot_values = {s: v.split('|')[0] for s, v in data_item['last_slot_values'].items()}\n",
    "    else:\n",
    "        last_slot_values = given_context\n",
    "    last_sys_utt = data_item['dialog']['sys'][-1]\n",
    "    if last_sys_utt == 'none':\n",
    "        last_sys_utt = ''\n",
    "    user_string = python_demo.get_user_string(data_item['dialog']['usr'][-1])\n",
    "    state_string, _ = python_demo.get_python_statements(last_slot_values, {},\n",
    "                                                        turn_strings=[last_sys_utt, user_string],\n",
    "                                                        detailed_state_string=detailed_state_string)\n",
    "    _, gt_string = python_demo.get_python_statements(last_slot_values, data_item['slot_values'],\n",
    "                                                        turn_strings=[last_sys_utt, user_string],\n",
    "                                                        detailed_state_string=detailed_state_string)\n",
    "    if not use_null_data_item:\n",
    "        user_msg += \"    \" + state_string + \"\\n\"\n",
    "        if last_sys_utt:\n",
    "            user_msg += \"    \" + python_demo.get_system_string(last_sys_utt) + \"\\n\"\n",
    "        user_msg += \"    \" + user_string + \"\\n\"\n",
    "    else:\n",
    "        pass  # default adds our null input at end\n",
    "    msg.append({\"role\": \"user\",\"content\": user_msg})\n",
    "    # msg.append({\"role\": \"assistant\",\"content\": \"    agent.state.\"})\n",
    "    msg[1]['content'] = 'import abc\\nfrom dataclasses import dataclass\\nfrom typing import Literal, Union\\n\\nPriceRange = Literal[\"dontcare\", \"cheap\", \"moderate\", \"expensive\"]\\nHotelType = Literal[\"hotel\", \"guest house\", \"dontcare\"]\\nOption = Literal[\"yes\", \"no\", \"dontcare\"]\\nDayOfWeek = Literal[\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\\nArea = Literal[\"dontcare\", \"centre\", \"east\", \"north\", \"south\", \"west\"]\\n\\n\\n@dataclass\\nclass Hotel:\\n    name: str = None\\n    price_range: PriceRange = None\\n    type: HotelType = None\\n    parking: Option = None\\n    book_number_of_days: int = None\\n    book_day: DayOfWeek = None\\n    book_people: int = None\\n    area: Area = None\\n    stars: Union[int, Literal[\"dontcare\"]] = None  # between 0 and 5 or dontcare\\n    internet: Option = None\\n\\n\\n@dataclass\\nclass Train:\\n    destination: str = None\\n    leave_from: str = None\\n    day: DayOfWeek = None\\n    book_people: int = None\\n    depart_time: str = None  # hh:mm or dontcare\\n    arrive_by_time: str = None  # hh:mm or dontcare\\n\\n\\nAttractionType = Literal[\"architecture\", \"boat\", \"church\", \"cinema\", \"college\", \"concert hall\", \"entertainment\",\\n                         \"hotspot\", \"multiple sports\", \"museum\", \"nightclub\", \"park\", \"special\", \"swimming pool\",\\n                         \"theatre\", \"dontcare\"]\\n\\n\\n@dataclass\\nclass Attraction:\\n    name: str = None\\n    area: Area = None\\n    type: AttractionType = None\\n\\n\\n@dataclass\\nclass Restaurant:\\n    name: str = None\\n    food_type: str = None\\n    price_range: PriceRange = None\\n    area: Area = None\\n    book_time: str = None  # hh:mm or dontcare\\n    book_day: DayOfWeek = None\\n    book_people: int = None\\n\\n\\n@dataclass\\nclass Taxi:\\n    destination: str = None\\n    leave_from: str = None\\n    depart_time: str = None  # hh:mm or dontcare\\n    arrive_by_time: str = None  # hh:mm or dontcare\\n\\n\\n@dataclass\\nclass BeliefState:\\n    hotel: Hotel = None\\n    train: Train = None\\n    attraction: Attraction = None\\n    restaurant: Restaurant = None\\n    taxi: Taxi = None\\n\\n\\nclass DialogueAgent(abc.ABC):\\n\\n    state: BeliefState\\n\\n    @abc.abstractmethod\\n    def find_hotel(self, name: str = None, price_range: PriceRange = None, type: HotelType = None,\\n                    parking: Option = None, book_number_of_days: int = None, book_day: DayOfWeek = None,\\n                    book_people: int = None, area: Area = None, stars: Union[int, Literal[\"dontcare\"]] = None,\\n                    internet: Option = None) -> Hotel:\\n        pass\\n\\n    @abc.abstractmethod\\n    def find_train(self, destination: str = None, leave_from: str = None, day: DayOfWeek = None,\\n                   book_people: int = None, depart_time: str = None, arrive_by_time: str = None) -> Train:\\n        pass\\n\\n    @abc.abstractmethod\\n    def find_attraction(self, name: str = None, area: Area = None, type: AttractionType = None) -> Attraction:\\n        pass\\n\\n    @abc.abstractmethod\\n    def find_restaurant(self, name: str = None, food_type: str = None, price_range: PriceRange = None,\\n                        area: Area = None, book_time: str = None, book_day: DayOfWeek = None,\\n                        book_people: int = None, ) -> Restaurant:\\n        pass\\n\\n    @abc.abstractmethod\\n    def find_taxi(self, destination: str = None, leave_from: str = None, depart_time: str = None,\\n                  arrive_by_time: str = None) -> Taxi:\\n        pass\\n\\n    def get_state(self) -> BeliefState:\\n        return self.state\\n\\n\\nif __name__ == \\'__main__\\':\\n    agent = DialogueAgent()\\n    state = BeliefState()\\n\\n' + msg[1]['content']\n",
    "    return msg, gt_string\n",
    "\n",
    "def make_two_type_msg(msg_chat):\n",
    "        \"\"\"Get the python Chat prompt for the query data and retrieved examples\n",
    "            msg_chat = [{'role':'system', 'content':'system_msg'}, \n",
    "            {'role':'user', 'content':'Some Instruction + Example 1 Input'}, \n",
    "            {'role':'assistant', 'content':'Example 1 Gold Output'},\n",
    "            {'role':'user', 'content':'Example 2 Input'}, \n",
    "            {'role':'assistant', 'content':'Example 2 Gold Output'},  ...\n",
    "            {'role':'user', 'content':'Example Target Input'}, \n",
    "            {'role':'assistant', 'content':'The answer is:    '}]\n",
    "            \n",
    "            msg_chat_usr = [{'role':'system', 'content':'system_msg'}, \n",
    "            {'role':'user', 'content':'Some Instruction + Example 1 Input'}, \n",
    "            {'role':'assistant', 'content':'Example 1 Gold Output'},\n",
    "            {'role':'user', 'content':'Example 2 Input'}, \n",
    "            {'role':'assistant', 'content':'Example 2 Gold Output'},  ...\n",
    "            {'role':'user', 'content':'Example Target Input + 'The answer is:    ''}]\n",
    "            \n",
    "            msg_one_prompt = [{'role':'system', 'content':'system_msg'}, \n",
    "            {'role':'user', \n",
    "                'content':'Some Instruction + Example 1 Input + Example 1 Gold Output + \\\n",
    "                        ... + Example Target Input + 'The answer is:    ''}], \n",
    "        \"\"\"\n",
    "\n",
    "        msg_chat_usr_last = copy.deepcopy(msg_chat)\n",
    "        msg_chat_usr_last[-2]['content'] += msg_chat_usr_last[-1]['content']\n",
    "        msg_chat_usr_last.pop()\n",
    "        msg_one_prompt = copy.deepcopy(msg_chat)\n",
    "        for _ in range(2, len(msg_one_prompt)):\n",
    "            msg_one_prompt[1]['content'] += msg_one_prompt[2]['content']\n",
    "            msg_one_prompt.pop(2)\n",
    "        \n",
    "        return msg_chat_usr_last, msg_one_prompt\n",
    "    \n",
    "with open('data/mw21_5p_train_v1.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data/mw24_20p_dev.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "idx1, idx2 = 0, 1\n",
    "exp_name_1 = experiments[idx1]['name']\n",
    "exp_name_2 = experiments[idx2]['name']\n",
    "\n",
    "A_right, B_right = experiments[idx1]['right'], experiments[idx2]['right']\n",
    "A_q_e_list, B_q_e_list = experiments[idx1]['right_shots'], experiments[idx2]['right_shots']\n",
    "assert A_right == [list(x.keys())[0] for x in A_q_e_list] and B_right == [list(x.keys())[0] for x in B_q_e_list]\n",
    "\n",
    "A_B = list(set(A_right).intersection(set(B_right)))\n",
    "A_Bc, Ac_B = list(set(A_right)-set(B_right)), list(set(B_right)-set(A_right))\n",
    "\n",
    "def make_prompt(train_data, eval_data, examples, A_list, B_list, exp_name_1, exp_name_2, prompt_list):\n",
    "    for query_id in examples:\n",
    "        tmp = {}\n",
    "        query_data = list(filter(lambda x: x[\"ID\"].replace('.json', '_')+str(x[\"turn_id\"]) == query_id, eval_data))[0]\n",
    "        \n",
    "        for exp_name, li in zip([exp_name_1, exp_name_2], [A_list, B_list]):\n",
    "            query_examples = list(filter(lambda x: list(x.keys())[0] == query_id, li))[0]\n",
    "            ex_list = list(query_examples.values())[0]\n",
    "            retreived_examples= [list(filter(lambda x: x[\"ID\"].replace('.json', '_')+str(x[\"turn_id\"]) == ex_id, train_data))[0] for ex_id in ex_list]\n",
    "\n",
    "            msg_chat, _ = get_python_chat_prompt(query_data, retreived_examples)    \n",
    "            _, msg_one_prompt = make_two_type_msg(msg_chat)\n",
    "            prompt = msg_one_prompt[1]['content']\n",
    "            tmp.update({'query':query_data, f'{exp_name}': prompt})\n",
    "\n",
    "        prompt_list.append(tmp)\n",
    "    return prompt_list\n",
    "\n",
    "# a_b_prompt1 = make_prompt(train_data, eval_data, A_B, experiments[idx1]['right_shots'], experiments[idx2]['right_shots'], exp_name_1, exp_name_2, [])\n",
    "# a_bc_prompt1 = make_prompt(train_data, eval_data, A_Bc, experiments[idx1]['right_shots'], experiments[idx2]['wrong_shots'], exp_name_1, exp_name_2, [])\n",
    "# ac_b_prompt1 = make_prompt(train_data, eval_data, Ac_B, experiments[idx1]['wrong_shots'], experiments[idx2]['right_shots'], exp_name_1, exp_name_2, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import openai\n",
    "\n",
    "from my_openai_key import get_openai_key\n",
    "openai.api_key = get_openai_key()\n",
    "\n",
    "with open('data/mw21_5p_train_v1.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data/mw24_20p_dev.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "normalizer = DataOntologyNormalizer(\n",
    "        Ontology.create_ontology(),\n",
    "        # count labels from the train set\n",
    "        supervised_set=train_data,\n",
    "        # make use of existing surface form knowledge encoded in ontology.json, released with each dataset\n",
    "        # see README.json within https://github.com/smartyfh/MultiWOZ2.4/raw/main/data/MULTIWOZ2.4.zip\n",
    "        counts_from_ontology_file=\"src/refpydst/db/multiwoz/2.4/ontology.json\"\n",
    ")\n",
    "\n",
    "experiment_id = 0\n",
    "num_smapled_examples = 5\n",
    "num_sampling = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./wrong_logs.json', 'w') as f:\n",
    "    json.dump(experiments[experiment_id]['wrong_logs'], f, indent=4)\n",
    "\n",
    "hard_queries = [i for i in set(experiments[experiment_id]['wrong'])-set(experiments[-1]['right'])]\n",
    "hard_queries = [list(filter(lambda x: list(x.keys())[0] == ex_id, experiments[experiment_id]['wrong_logs']))[0] for ex_id in hard_queries]\n",
    "with open('./zero_right_fine_tuned_sbert_wrong.json', 'w') as f:\n",
    "    json.dump(hard_queries, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_queries = [i for i in set(experiments[experiment_id]['right'])-set(experiments[-1]['right'])]\n",
    "hard_queries = [list(filter(lambda x: list(x.keys())[0] == ex_id, experiments[experiment_id]['right_logs']))[0] for ex_id in hard_queries]\n",
    "\n",
    "wrong_queries = experiments[experiment_id]['wrong_logs']\n",
    "total_logs = []\n",
    "for epoch, query_shots in enumerate(wrong_queries):\n",
    "    # raise ValueError\n",
    "    # query_shots = experiments[0]['right_shots'][0]\n",
    "    query_id, logs = list(query_shots.items())[0]\n",
    "    example_set = logs['examples']\n",
    "\n",
    "    query_data = list(filter(lambda x: x[\"ID\"].replace('.json', '_')+str(x[\"turn_id\"]) == query_id, eval_data))[0]\n",
    "\n",
    "    retrieved_examples = []\n",
    "    for example_id in example_set:\n",
    "        example = list(filter(lambda x: x[\"ID\"].replace('.json', '_')+str(x[\"turn_id\"]) == example_id, train_data))[0]\n",
    "        retrieved_examples.append(example)\n",
    "\n",
    "    sampling_pool = []\n",
    "    for _ in range(num_sampling):\n",
    "        sampling_pool += random.sample(retrieved_examples, len(retrieved_examples))\n",
    "\n",
    "    _, gold_state_python = python_demo.get_python_statements(query_data['last_slot_values'], query_data['slot_values'], \n",
    "                                                            turn_strings=[\n",
    "                                                                 query_data['dialog']['sys'][-1],\n",
    "                                                                 python_demo.get_user_string(query_data['dialog']['usr'][-1])],\n",
    "                                                            detailed_state_string=True)\n",
    "\n",
    "    results = {\n",
    "        'experiment': experiments[experiment_id]['name'],\n",
    "        'ID': query_data['ID'], 'turn_id': query_data['turn_id'],\n",
    "        'dialog': query_data['dialog'], 'slot_values': query_data['slot_values'], 'pred_slot_values':logs['pred'],\n",
    "        'turn_slot_values': query_data['turn_slot_values'], 'pred_turn_slot_values': logs['pred_delta_slot_values'],\n",
    "        'gold_python': gold_state_python, 'pred_python': logs['completion'], 'examples': example_set \n",
    "    }\n",
    "    \n",
    "    results['sampling_scoring_exp'] = {}\n",
    "    for it in range(num_sampling):\n",
    "    #     random.shuffle(retrieved_examples)\n",
    "        \n",
    "        for step, idx in enumerate(range(0, len(retrieved_examples), num_smapled_examples)):\n",
    "            # for step, idx in enumerate(range(0, len(sampling_pool), num_smapled_examples)):\n",
    "            sampled_expamples = sampling_pool[num_smapled_examples*2*it+idx: num_smapled_examples*(2*it+1)+idx]\n",
    "            msg_chat, gold_python = get_python_chat_prompt(query_data,sampled_expamples)\n",
    "                \n",
    "            args = {\n",
    "                \"model\": 'gpt-3.5-turbo',\n",
    "                \"messages\": msg_chat,\n",
    "                \"max_tokens\": 120,\n",
    "                \"temperature\": 0.0,\n",
    "                \"stop\": ['--', '\\n', ';', '#'],\n",
    "                \"logprobs\": True,  # 1 needed to get back log-probabilities at all, in choice['logprobs']['token_logprobs']\n",
    "            }\n",
    "            # result = 100 * it\n",
    "            result = openai.chat.completions.create(**args)\n",
    "\n",
    "            completions = dict(zip(\n",
    "                [x.message.content for x in result.choices],\n",
    "                [sum(token.logprob for token in x.logprobs.content) for x in result.choices]\n",
    "            ))\n",
    "            best_completion = max(completions, key=completions.get)\n",
    "            best_completion = best_completion.strip().replace('agent.state.', '')\n",
    "            \n",
    "            results['sampling_scoring_exp'].update({\n",
    "                f'iteration_{it}_step_{step}': {\n",
    "                    'completions': completions, 'best_completion': best_completion, 'sampled_examples': list(map(lambda x: x['ID'].replace('.json', '_')+str(x['turn_id']), sampled_expamples))\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    results['scores'] = {}\n",
    "    results['scores']['score_delta'] = {ids: 0 for ids in example_set}\n",
    "    results['scores']['score_full'] = {ids: 0 for ids in example_set}\n",
    "    results['scores']['occurrences'] = {ids: 0 for ids in example_set}\n",
    "\n",
    "    for iter_step, log in results['sampling_scoring_exp'].items():\n",
    "        for ex_id in log['sampled_examples']:\n",
    "            results['scores']['occurrences'][ex_id] += 1\n",
    "        \n",
    "        pred_delta = parse_python_completion(log['best_completion'], {})\n",
    "        pred_delta = normalizer.normalize(pred_delta)\n",
    "        if pred_delta == results['turn_slot_values']:\n",
    "            for ex_id in log['sampled_examples']:\n",
    "                results['scores']['score_delta'][ex_id] += 1\n",
    "        \n",
    "        pred_full = update_dialogue_state(query_data['last_slot_values'], pred_delta)\n",
    "        if pred_full == results['slot_values']:\n",
    "            for ex_id in log['sampled_examples']:\n",
    "                results['scores']['score_full'][ex_id] += 1\n",
    "    \n",
    "    total_logs.append(results)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        with open('./total_logs.json', 'a') as f:\n",
    "            json.dump(total_logs, f, indent=4)\n",
    "    \n",
    "# with open('./total_logs.json', 'a') as f:\n",
    "#     json.dump(total_logs, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./t.json', 'r') as f:\n",
    "    total_logs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "error_accumulation = []\n",
    "for logs in experiments[0]['wrong_logs']:\n",
    "    key = list(logs.keys())[0]\n",
    "    query = logs[key]\n",
    "    new_pred = parse_python_completion(query['completion'], query['last_slot_values'])\n",
    "    new_pred = normalizer.normalize(new_pred)\n",
    "    if query['pred']!=query['slot_values']:\n",
    "        wrong+=1\n",
    "    if new_pred == query['slot_values'] and query['pred'] != query['slot_values']:\n",
    "        error_accumulation.append(query['ID'].replace('.json', '_')+str(query['turn_id']))\n",
    "print()\n",
    "print(len(experiments[0]['wrong_logs']), len(error_accumulation), len(experiments[0]['wrong_logs'])- len(error_accumulation))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hall = []\n",
    "slot_hall = []\n",
    "confusion = []\n",
    "missing = []\n",
    "right = []\n",
    "for log in experiments[0]['wrong_logs']:\n",
    "    key = list(log.keys())[0]\n",
    "    log[key]['slot_values'] = dict(sorted(log[key]['slot_values'].items(), key=lambda x: x[0]))\n",
    "    log[key]['pred'] = dict(sorted(log[key]['pred'].items(), key=lambda x: x[0]))\n",
    "    pred = log[key]['pred']\n",
    "    gold = log[key]['slot_values']\n",
    "    # pred_slot hallucination\n",
    "    if set(pred.values()) - set(gold.values()):\n",
    "        hall.append(log)\n",
    "    elif set(gold.values()) - set(pred.values()):\n",
    "        missing.append(log)\n",
    "    else:\n",
    "        if pred.keys() != gold.keys():\n",
    "            slot_hall.append(log)\n",
    "        else:\n",
    "            if pred == gold:\n",
    "                right.append(log)   \n",
    "            else:\n",
    "                confusion.append(log)\n",
    "print()\n",
    "print(len(hall), len(missing), len(slot_hall), len(confusion))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hall = []\n",
    "slot_hall = []\n",
    "confusion = []\n",
    "missing = []\n",
    "right = []\n",
    "\n",
    "for log in experiments[0]['wrong_logs']:\n",
    "    key = list(log.keys())[0]\n",
    "    if key not in set(experiments[0]['wrong'])-set(error_accumulation):\n",
    "        continue\n",
    "    log[key]['slot_values'] = dict(sorted(log[key]['slot_values'].items(), key=lambda x: x[0]))\n",
    "    log[key]['pred'] = dict(sorted(log[key]['pred'].items(), key=lambda x: x[0]))\n",
    "    pred = log[key]['pred']\n",
    "    gold = log[key]['slot_values']\n",
    "    # pred_slot hallucination\n",
    "    if set(pred.values()) - set(gold.values()):\n",
    "        hall.append(log)\n",
    "    elif set(gold.values()) - set(pred.values()):\n",
    "        missing.append(log)\n",
    "    else:\n",
    "        if pred.keys() != gold.keys():\n",
    "            slot_hall.append(log)\n",
    "        else:\n",
    "            if pred == gold:\n",
    "                right.append(log)   \n",
    "            else:\n",
    "                confusion.append(log)\n",
    "print()\n",
    "print(len(hall), len(missing), len(slot_hall), len(confusion))\n",
    "print()\n",
    "# 244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_accumulation = []\n",
    "for logs in experiments[2]['wrong_logs']:\n",
    "    key = list(logs.keys())[0]\n",
    "    query = logs[key]\n",
    "    pred = parse_python_completion(query['completion'], query['last_slot_values'])\n",
    "    pred = normalizer.normalize(pred)\n",
    "    \n",
    "    if pred == query['slot_values'] and query['pred'] != query['slot_values']:\n",
    "        error_accumulation.append(query['ID'].replace('.json', '_')+str(query['turn_id']))\n",
    "print()\n",
    "print(len(experiments[2]['wrong_logs']), len(error_accumulation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hall = []\n",
    "slot_hall = []\n",
    "confusion = []\n",
    "missing = []\n",
    "right = []\n",
    "for log in experiments[2]['wrong_logs']:\n",
    "    key = list(log.keys())[0]\n",
    "    log[key]['slot_values'] = dict(sorted(log[key]['slot_values'].items(), key=lambda x: x[0]))\n",
    "    log[key]['pred'] = dict(sorted(log[key]['pred'].items(), key=lambda x: x[0]))\n",
    "    pred = log[key]['pred']\n",
    "    gold = log[key]['slot_values']\n",
    "    # pred_slot hallucination\n",
    "    if set(pred.values()) - set(gold.values()):\n",
    "        hall.append(log)\n",
    "    elif set(gold.values()) - set(pred.values()):\n",
    "        missing.append(log)\n",
    "    else:\n",
    "        if pred.keys() != gold.keys():\n",
    "            slot_hall.append(log)\n",
    "        else:\n",
    "            if pred == gold:\n",
    "                right.append(log)   \n",
    "            else:\n",
    "                confusion.append(log)\n",
    "\n",
    "len(hall), len(missing), len(slot_hall), len(confusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hall = []\n",
    "slot_hall = []\n",
    "confusion = []\n",
    "missing = []\n",
    "right = []\n",
    "for log in experiments[2]['wrong_logs']:\n",
    "    key = list(log.keys())[0]\n",
    "    if key not in set(experiments[2]['wrong'])-set(error_accumulation):\n",
    "        continue\n",
    "    log[key]['slot_values'] = dict(sorted(log[key]['slot_values'].items(), key=lambda x: x[0]))\n",
    "    log[key]['pred'] = dict(sorted(log[key]['pred'].items(), key=lambda x: x[0]))\n",
    "    pred = log[key]['pred']\n",
    "    gold = log[key]['slot_values']\n",
    "    # pred_slot hallucination\n",
    "    if set(pred.values()) - set(gold.values()):\n",
    "        hall.append(log)\n",
    "    elif set(gold.values()) - set(pred.values()):\n",
    "        missing.append(log)\n",
    "    else:\n",
    "        if pred.keys() != gold.keys():\n",
    "            slot_hall.append(log)\n",
    "        else:\n",
    "            if pred == gold:\n",
    "                right.append(log)   \n",
    "            else:\n",
    "                confusion.append(log)\n",
    "\n",
    "len(hall), len(missing), len(slot_hall), len(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib_venn import venn2\n",
    "\n",
    "# fig, axes = plt.subplots(num_exps,3, figsize=(15, 15))\n",
    "# r, c = 0, 0\n",
    "# tmp = defaultdict(dict)\n",
    "# for idx1 in range(len(experiments)):\n",
    "#     exp_name_1 = experiments[idx1]['name']\n",
    "#     right_1 = experiments[idx1]['right']\n",
    "#     for idx2 in range(idx1+1, len(experiments)):    \n",
    "#         exp_name_2 = experiments[idx2]['name']\n",
    "#         right_2 = experiments[idx2]['right']\n",
    "        \n",
    "#         A, B = set(right_1), set(right_2)\n",
    "#         A_B = set(right_1).intersection(set(right_2))\n",
    "#         A_Bc = set(right_1)-set(right_2)\n",
    "#         Ac_B = set(right_2)-set(right_1)\n",
    "    \n",
    "#         a_b, a_bc, ac_b = len(A_B), len(A_Bc), len(Ac_B)\n",
    "#         total = a_b+a_bc+ac_b\n",
    "#         a_b /= len(experiments[0]['shots'])\n",
    "#         a_bc /= len(experiments[0]['shots'])\n",
    "#         ac_b /= len(experiments[0]['shots'])\n",
    "#         # a_b /= total\n",
    "#         # a_bc /= total\n",
    "#         # ac_b /= total\n",
    "#         tmp[exp_name_1][exp_name_2] = sum([round(a_bc,4), round(ac_b,4), round(a_b,4)])\n",
    "#         # venn2([A, B], (f'{exp_name_1}',f'{exp_name_2}'), ax=axes[r][c])\n",
    "#         venn2([round(a_bc,4), round(ac_b,4), round(a_b,4)], (f'{exp_name_1}',f'{exp_name_2}'), ax=axes[r][c])\n",
    "#         c+=1\n",
    "#         if c >= 3:\n",
    "#             r+=1\n",
    "#             c=0\n",
    "    \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(7, 7))\n",
    "# sns.heatmap(pd.DataFrame(tmp), annot=True, cmap='coolwarm', fmt=\".4f\")\n",
    "# # 두 개의 retreival method의 맞춘 query를 모두 더하여, ideal한 점수를 내었을 때 최고 점은 0.6717로, 기존 최고점 보다 10 p 정도 높음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(num_exps,num_exps, figsize=(22, 22))\n",
    "# r, c = 0, 0\n",
    "\n",
    "# for idx1 in range(len(experiments)):\n",
    "#     exp_name_1 = experiments[idx1]['name']\n",
    "#     for idx2 in range(len(experiments)):\n",
    "#         exp_name_2 = experiments[idx2]['name']\n",
    "#         overlap_ratio = copy.deepcopy(np.array(overlap_map[exp_name_1][exp_name_2]).astype(float))\n",
    "#         overlap_ratio /= overlap_ratio.sum(1)[:,None].astype(float)\n",
    "#         overlap_ratio = (overlap_ratio.sum(0)/overlap_ratio.shape[0]).round(4)\n",
    "#         overlap_total = copy.deepcopy(np.array(overlap_map[exp_name_1][exp_name_2]).astype(float)).sum(0)\n",
    "\n",
    "#         # if exp_name_1 != exp_name_2:\n",
    "#         arr = overlap_ratio # overlap_total / overlap_ratio\n",
    "#         v = venn2((arr[1], arr[2], arr[0]), (f'{exp_name_1}',f'{exp_name_2}'), ax=axes[r][c])\n",
    "#         for lab in v.set_labels:\n",
    "#             lab.set_rotation(45)\n",
    "#         c+=1\n",
    "#         if c >= num_exps:\n",
    "#             r+=1\n",
    "#             c=0\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
