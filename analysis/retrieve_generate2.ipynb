{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from my_funcs import (\n",
    "    default_transformation, read_json_from_data_dir,\n",
    "    normalize, embed_query_retrieve_examples, iterate_nearest_dialogs,\n",
    "    make_two_type_msg, get_python_chat_prompt, \n",
    "    parse_python_completion, update_dialogue_state, \n",
    "    compute_acc, calculate_token_f1, evaluate,\n",
    "    DataOntologyNormalizer, Ontology,\n",
    "    copy, defaultdict, random,\n",
    "    openai, tiktoken, SentenceTransformer\n",
    ")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "# from rank_bm25 import BM25Okapi\n",
    "from refpydst.prompt_formats.python.completion_parser import *\n",
    "from refpydst.prompt_formats.python.completion_parser import parse_python_modified\n",
    "from refpydst.evaluate_metrics import evaluate\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from my_openai_key import get_openai_key\n",
    "openai.api_key = get_openai_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query_retrieve_examples(embedder, example_pool, query_data, emb_keys, emb_values, label_to_idx, num_retrieved_examples=10):\n",
    "    # Embed Query based on its turn and previous (predicted) slot values \n",
    "    with torch.no_grad():\n",
    "        query_string = default_transformation(query_data)\n",
    "        query_emb = embedder.encode(query_string, convert_to_numpy=True).reshape(1, -1)\n",
    "    \n",
    "    exmple_generator = (\n",
    "            (example, score)\n",
    "            for turn_label, score in iterate_nearest_dialogs(query_emb, emb_keys, emb_values, k=5)\n",
    "                for example in example_pool\n",
    "                    if example['ID'] == turn_label.split('_')[0] and example['turn_id'] == int(turn_label.split('_')[-1])\n",
    "        )\n",
    "\n",
    "    all_considered_examples: List[Tuple[Turn, float]] = \\\n",
    "        [turn_and_score for _, turn_and_score in zip(range(100), exmple_generator)]\n",
    "    all_embeddings = np.asarray([\n",
    "        emb_values[label_to_idx[f\"{turn['ID']}_turn_{turn['turn_id']}\"]]\n",
    "        for turn, score in all_considered_examples\n",
    "    ])\n",
    "    if len(all_considered_examples) == 0:\n",
    "        raise ValueError(\"No examples found in the retriever index.\")\n",
    "\n",
    "    result: List[int] = []\n",
    "    example_scores = np.asarray([1 - 0.5*(score**2) for turn, score in all_considered_examples])\n",
    "    assert np.all(np.diff(example_scores) <= 0)  # verifies they are decreasing as expected\n",
    "    while len(result) < num_retrieved_examples:\n",
    "        best_idx: int = np.argmax(example_scores).item()\n",
    "        example_scores[best_idx] = -np.inf\n",
    "        result.append(best_idx)\n",
    "        best_emb = all_embeddings[best_idx]\n",
    "        discount = 0.2 * cosine_similarity(best_emb[None, :], all_embeddings).squeeze(0)\n",
    "        example_scores = example_scores - discount\n",
    "\n",
    "    retrieved_exampels = [all_considered_examples[i][0] for i in result][::-1]\n",
    "    retrieved_exampels = [e for e in retrieved_exampels if e['ID'] != query_data['ID']][-10:]\n",
    "    return retrieved_exampels\n",
    "\n",
    "def make_dict(query_data):\n",
    "    li = []    \n",
    "    for generated_example in query_data['generated']:\n",
    "        tmp = {}\n",
    "        tmp['dialog'] = {}\n",
    "\n",
    "        if '[context]' in generated_example:\n",
    "            contxt_end = generated_example.index('[context]') + len('[context]')\n",
    "        else:\n",
    "            contxt_end = 0\n",
    "    \n",
    "        if '[utterance_sys]' in generated_example:\n",
    "            sys_start = generated_example.index('[utterance_sys]')\n",
    "            sys_end = sys_start + len('[utterance_sys]')\n",
    "        else:\n",
    "            sys_start = contxt_end+1\n",
    "            sys_end = sys_start\n",
    "    \n",
    "        if '[utterance_usr]' in generated_example:\n",
    "            usr_start = generated_example.index('[utterance_usr]')\n",
    "            usr_end = usr_start + len('[utterance_usr]')\n",
    "        else:\n",
    "            usr_start = sys_end+1\n",
    "            usr_end = usr_start\n",
    "        \n",
    "        if '[belief_state]' in generated_example:\n",
    "            belief_start = generated_example.index('[belief_state]')\n",
    "            belief_end = belief_start + len('[belief_state]')\n",
    "        else:\n",
    "            belief_start = None\n",
    "            belief_end = None\n",
    "\n",
    "        tmp['last_slot_values'] = dict(eval('{'+generated_example[contxt_end:sys_start]+'}'))        \n",
    "        tmp['dialog']['sys'] = [generated_example[sys_end:usr_start]]\n",
    "        tmp['dialog']['usr'] = [generated_example[usr_end:belief_start]]\n",
    "\n",
    "        tmp['slot_values'] = {}\n",
    "        bs = generated_example[belief_end:].strip() if belief_end is not None else ''\n",
    "        for slot_val in bs.split(','):\n",
    "            s_v_list = slot_val.split(':')\n",
    "            if len(s_v_list) <= 1:\n",
    "                continue\n",
    "            if len(s_v_list) > 2:\n",
    "                s_v_list[1] = ':'.join(s_v_list[1:])\n",
    "            slot, val = s_v_list[0].strip(), s_v_list[1].strip()\n",
    "            tmp['slot_values'][slot] = val\n",
    "        li.append(tmp)\n",
    "    return li\n",
    "\n",
    "query_data_path = 'jun/inference/mw24_20p_dev_wrong_67.json'\n",
    "\n",
    "with open(query_data_path, 'r') as f:\n",
    "    query_dataset = json.load(f)\n",
    "new_data = []\n",
    "for query_data in query_dataset:\n",
    "    new_data.append(make_dict(query_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/llama/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[7], line 3\u001b[0m\n    new_data.append(make_dict(query_data))\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 73\u001b[0;36m in \u001b[0;35mmake_dict\u001b[0;36m\n\u001b[0;31m    tmp['slot_values'] = dict(eval('{'+ generated_example[belief_end:]+'}'))\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    { that s all i need today }\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pool_path = 'data/mw21_100p_train.json'\n",
    "query_data_path = 'jun/inference/mw24_20p_dev_wrong_67.json'\n",
    "engine = \"/data1/home/haesungpyun/models/Meta-Llama-3-70B-Instruct-GPTQ\"\n",
    "quantization = 'gptq'\n",
    "\n",
    "with open(query_data_path, 'r') as f:\n",
    "    query_dataset = json.load(f)\n",
    "with open(sample_pool_path, 'r') as f:\n",
    "    sample_pool = json.load(f)\n",
    "\n",
    "# Register all dialogues from the train dataset to example pool and Get all the unique dialogue ids in example pool\n",
    "example_pool = []\n",
    "selected_dialog_id_from_split = set()\n",
    "for dataset in [sample_pool]:\n",
    "    example_pool += dataset\n",
    "    selected_dialog_id_from_split.update([dial['ID'] for dial in dataset])\n",
    "\n",
    "# Load the all train data index\n",
    "retriever_full_path = '/home/haesungpyun/my_refpydst/outputs/runs/retriever/mw21_100p_train/referred_states'\n",
    "search_index_filename = os.path.join(retriever_full_path, \"train_index.npy\")\n",
    "search_embeddings = np.load(search_index_filename, allow_pickle=True).item()    # {'MUL0720.json_turn_10': np.array([0.1, 0.2, ...]), ...}\n",
    "\n",
    "# Keep only embeddings for the selected dialogues in split version\n",
    "emb_dict = {k: v for k, v in search_embeddings.items() if k.split('_')[0] in selected_dialog_id_from_split}\n",
    "emb_keys = list(emb_dict.keys())\n",
    "emb_dim = emb_dict[emb_keys[0]].shape[-1]\n",
    "\n",
    "# Convert embeddings to array and Normalize them\n",
    "emb_values = np.zeros((len(emb_keys), emb_dim))\n",
    "for i, k in enumerate(emb_keys):\n",
    "    emb_values[i] = emb_dict[k]\n",
    "emb_values = normalize(emb_values)\n",
    "\n",
    "# Create a label to index mapping  {'MUL0720.json_turn_10': 1, ...} \n",
    "label_to_idx = {k: i for i, k in enumerate(emb_keys)}\n",
    "\n",
    "# Load the model for embed query\n",
    "embedder = SentenceTransformer(retriever_full_path)\n",
    "\n",
    "# Tokenizer\n",
    "# encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "\n",
    "model = LLM(model=\"/data1/home/haesungpyun/models/Meta-Llama-3-70B-Instruct-GPTQ\", quantization='gptq', enforce_eager=False)\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "terminators =  [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")    \n",
    "]\n",
    "\n",
    "stop_sequences = ['--', '\\n', ';', '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query에 대해 retrieve & generate & evaluate\n",
    "total_log = []\n",
    "retrieving_samples = True\n",
    "num_retrieved_examples = 100\n",
    "n_smapled_examples = 10\n",
    "\n",
    "random.seed(42)\n",
    "# Randomly select a query data and Retrieve examples from example pool (train data)\n",
    "# query_data = query_dataset[random.randint(0, len(dev_dataset))]\n",
    "\n",
    "prev_data = None\n",
    "for query_data in query_dataset:\n",
    "    query_data['pred_prior_context'] = prev_data['pred_slot_values'] if prev_data else {}\n",
    "    modified_data = copy.deepcopy(query_data)\n",
    "    modified_data['last_slot_values'] = query_data.get('pred_prior_context', {})\n",
    "\n",
    "    retrieved_examples = embed_query_retrieve_examples(\n",
    "        embedder, example_pool, modified_data, \n",
    "        emb_keys, emb_values, label_to_idx, num_retrieved_examples=num_retrieved_examples)\n",
    "    msg_chat, gold_python = get_python_chat_prompt(query_data, retrieved_examples) \n",
    "\n",
    "    generated_examples = make_dict(query_data)\n",
    "    gen_msg_chat, _ = get_python_chat_prompt(query_data, generated_examples)   \n",
    "    # msg_chat_usr_last, msg_one_prompt = make_two_type_msg(msg_chat)\n",
    "    # raise ValueError    \n",
    "    log = defaultdict(dict)\n",
    "    log['ID-turn-id']= f\"{query_data['ID']}-{query_data['turn_id']}\"\n",
    "    log['last_slot_values'] = query_data['last_slot_values'] \n",
    "    log['turn_slot_values'] = query_data['turn_slot_values']\n",
    "    log['slot_values']= query_data['slot_values']\n",
    "    log['gold-python'] = gold_python\n",
    "\n",
    "    examples_list = []\n",
    "    for idx, example in enumerate(retrieved_examples):\n",
    "        tmp = {}\n",
    "        tmp['ID_turn-id'] = f\"{example['ID']}-{example['turn_id']}\"\n",
    "        tmp['last_slot_values'], tmp['turn_slot_values'], tmp['slot_values'] = \\\n",
    "            example['last_slot_values'], example['turn_slot_values'], example['slot_values']\n",
    "        examples_list.append(tmp)\n",
    "    log[\"retrieve_example\"] = examples_list\n",
    "\n",
    "    samplig_params = SamplingParams(\n",
    "        n=1, best_of=1, max_tokens=120, \n",
    "        temperature=0, stop=stop_sequences,\n",
    "        stop_token_ids=terminators)\n",
    "    msg_chat_ids = tokenizer.apply_chat_template(\n",
    "        msg_chat, add_generation_prompt=True, return_tensors='pt')\n",
    "    gen_msg_chat = tokenizer.apply_chat_template(\n",
    "        gen_msg_chat, add_generation_prompt=True, return_tensors='pt')\n",
    "    \n",
    "    prompts = [tokenizer.batch_decode(ids, skip_special_tokens=False)[0] for ids in [msg_chat_ids, gen_msg_chat]]\n",
    "    result = model.generate(prompts, sampling_params=samplig_params)\n",
    "    \n",
    "    completions = [{output.outputs[0].text: 1} for output in result]\n",
    "    best_completion = [comp.strip().replace('agent.state.', '') for dic in completions for comp,_ in dic.items()]\n",
    "\n",
    "    predicted_prior_context = query_data.get('pred_prior_context', query_data['last_slot_values'])\n",
    "    batch_pred_s_v = [parse_python_completion(comp, predicted_prior_context) for comp in best_completion]\n",
    "    batch_pred_turn_s_v = [parse_state_change(comp, predicted_prior_context) for comp in best_completion]\n",
    "\n",
    "################################################################################\n",
    "    pred_prev = prev_data.get('pred_slot_values', {}) if prev_data else {}\n",
    "    pred = update_dialogue_state(pred_prev, batch_pred_turn_s_v[0])\n",
    "\n",
    "    query_data['og_completion'] = best_completion[0]\n",
    "    query_data['og_pred_turn_slot_values'] = batch_pred_turn_s_v[0]\n",
    "    query_data['og_pred_slot_values'] = batch_pred_s_v[0]\n",
    "    \n",
    "    log['og_completion'] = best_completion[0]\n",
    "    log['og_pred_turn_slot_values'] = batch_pred_turn_s_v[0]\n",
    "    log['og_pred_slot_values'] = batch_pred_s_v[0]\n",
    "    \n",
    "    this_jga, this_acc, this_f1 = evaluate(pred, query_data['slot_values'])\n",
    "    delta_jga, _, _ = evaluate(batch_pred_turn_s_v[0], query_data['turn_slot_values'])\n",
    "    log['og_full_jga'] = this_jga\n",
    "    log['og_delta_jga'] = delta_jga\n",
    "\n",
    "###############################################################################\n",
    "    pred_prev = prev_data.get('pred_slot_values', {}) if prev_data else {}\n",
    "    pred = update_dialogue_state(pred_prev, batch_pred_turn_s_v[1])\n",
    "\n",
    "    query_data['completion'] = best_completion[1]\n",
    "    query_data['pred_turn_slot_values'] = batch_pred_turn_s_v[1]\n",
    "    query_data['pred_slot_values'] = batch_pred_s_v[1]\n",
    "    \n",
    "    log['completion'] = best_completion[1]\n",
    "    log['pred_turn_slot_values'] = batch_pred_turn_s_v[1]\n",
    "    log['pred_slot_values'] = batch_pred_s_v[1]\n",
    "    \n",
    "    this_jga, this_acc, this_f1 = evaluate(pred, query_data['slot_values'])\n",
    "    delta_jga, _, _ = evaluate(batch_pred_turn_s_v[1], query_data['turn_slot_values'])\n",
    "    log['full_jga'] = this_jga\n",
    "    log['delta_jga'] = delta_jga\n",
    "\n",
    "    total_log.append(log)\n",
    "\n",
    "    prev_data = query_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query에 대해 retrieve & generate & evaluate\n",
    "total_log = []\n",
    "retrieving_samples = True\n",
    "num_retrieved_examples = 100\n",
    "n_smapled_examples = 10\n",
    "\n",
    "random.seed(42)\n",
    "# Randomly select a query data and Retrieve examples from example pool (train data)\n",
    "# query_data = query_dataset[random.randint(0, len(dev_dataset))]\n",
    "\n",
    "prev_data = None\n",
    "for query_data in query_dataset:\n",
    "    query_data['pred_prior_context'] = prev_data['pred_slot_values'] if prev_data else {}\n",
    "    modified_data = copy.deepcopy(query_data)\n",
    "    modified_data['last_slot_values'] = query_data.get('pred_prior_context', {})\n",
    "\n",
    "    retrieved_examples = embed_query_retrieve_examples(\n",
    "        embedder, example_pool, modified_data, \n",
    "        emb_keys, emb_values, label_to_idx, num_retrieved_examples=num_retrieved_examples)\n",
    "    msg_chat, gold_python = get_python_chat_prompt(query_data, retrieved_examples) \n",
    "\n",
    "    generated_examples = make_dict(query_data)\n",
    "    gen_msg_chat, _ = get_python_chat_prompt(query_data, generated_examples)   \n",
    "    # msg_chat_usr_last, msg_one_prompt = make_two_type_msg(msg_chat)\n",
    "    # raise ValueError    \n",
    "    log = defaultdict(dict)\n",
    "    log['ID-turn-id']= f\"{query_data['ID']}-{query_data['turn_id']}\"\n",
    "    log['last_slot_values'] = query_data['last_slot_values'] \n",
    "    log['turn_slot_values'] = query_data['turn_slot_values']\n",
    "    log['slot_values']= query_data['slot_values']\n",
    "    log['gold-python'] = gold_python\n",
    "\n",
    "    examples_list = []\n",
    "    for idx, example in enumerate(retrieved_examples):\n",
    "        tmp = {}\n",
    "        tmp['ID_turn-id'] = f\"{example['ID']}-{example['turn_id']}\"\n",
    "        tmp['last_slot_values'], tmp['turn_slot_values'], tmp['slot_values'] = \\\n",
    "            example['last_slot_values'], example['turn_slot_values'], example['slot_values']\n",
    "        examples_list.append(tmp)\n",
    "    log[\"retrieve_example\"] = examples_list\n",
    "\n",
    "    samplig_params = SamplingParams(\n",
    "        n=1, best_of=1, max_tokens=120, \n",
    "        temperature=0, stop=stop_sequences,\n",
    "        stop_token_ids=terminators)\n",
    "    msg_chat_ids = tokenizer.apply_chat_template(\n",
    "        msg_chat, add_generation_prompt=True, return_tensors='pt')\n",
    "    gen_msg_chat = tokenizer.apply_chat_template(\n",
    "        gen_msg_chat, add_generation_prompt=True, return_tensors='pt')\n",
    "    \n",
    "    prompts = [tokenizer.batch_decode(ids, skip_special_tokens=False)[0] for ids in [msg_chat_ids, gen_msg_chat]]\n",
    "    result = model.generate(prompts, sampling_params=samplig_params)\n",
    "    \n",
    "    completions = [{output.outputs[0].text: 1} for output in result]\n",
    "    best_completion = [comp.strip().replace('agent.state.', '') for dic in completions for comp,_ in dic.items()]\n",
    "\n",
    "    predicted_prior_context = query_data.get('pred_prior_context', query_data['last_slot_values'])\n",
    "    batch_pred_s_v = [parse_python_completion(comp, predicted_prior_context) for comp in best_completion]\n",
    "    batch_pred_turn_s_v = [parse_state_change(comp, predicted_prior_context) for comp in best_completion]\n",
    "\n",
    "################################################################################\n",
    "    pred_prev = prev_data.get('pred_slot_values', {}) if prev_data else {}\n",
    "    pred = update_dialogue_state(pred_prev, batch_pred_turn_s_v[0])\n",
    "\n",
    "    query_data['og_completion'] = best_completion[0]\n",
    "    query_data['og_pred_turn_slot_values'] = batch_pred_turn_s_v[0]\n",
    "    query_data['og_pred_slot_values'] = batch_pred_s_v[0]\n",
    "    \n",
    "    log['og_completion'] = best_completion[0]\n",
    "    log['og_pred_turn_slot_values'] = batch_pred_turn_s_v[0]\n",
    "    log['og_pred_slot_values'] = batch_pred_s_v[0]\n",
    "    \n",
    "    this_jga, this_acc, this_f1 = evaluate(pred, query_data['slot_values'])\n",
    "    delta_jga, _, _ = evaluate(batch_pred_turn_s_v[0], query_data['turn_slot_values'])\n",
    "    log['og_full_jga'] = this_jga\n",
    "    log['og_delta_jga'] = delta_jga\n",
    "\n",
    "###############################################################################\n",
    "    pred_prev = prev_data.get('pred_slot_values', {}) if prev_data else {}\n",
    "    pred = update_dialogue_state(pred_prev, batch_pred_turn_s_v[1])\n",
    "\n",
    "    query_data['completion'] = best_completion[1]\n",
    "    query_data['pred_turn_slot_values'] = batch_pred_turn_s_v[1]\n",
    "    query_data['pred_slot_values'] = batch_pred_s_v[1]\n",
    "    \n",
    "    log['completion'] = best_completion[1]\n",
    "    log['pred_turn_slot_values'] = batch_pred_turn_s_v[1]\n",
    "    log['pred_slot_values'] = batch_pred_s_v[1]\n",
    "    \n",
    "    this_jga, this_acc, this_f1 = evaluate(pred, query_data['slot_values'])\n",
    "    delta_jga, _, _ = evaluate(batch_pred_turn_s_v[1], query_data['turn_slot_values'])\n",
    "    log['full_jga'] = this_jga\n",
    "    log['delta_jga'] = delta_jga\n",
    "\n",
    "    total_log.append(log)\n",
    "\n",
    "    prev_data = query_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.1_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
