/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
wandb: Currently logged in as: hacastle12. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.11
wandb: Run data is saved locally in /home/haesungpyun/my_refpydst/wandb/run-20240828_232411-xb1rlpe9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run -runs-instance_ad_hoc-8B-bm25-tf_idf
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hacastle12/refpydst
wandb: üöÄ View run at https://wandb.ai/hacastle12/refpydst/runs/xb1rlpe9
mapping supervised_set surface forms...:   0%|          | 0/2731 [00:00<?, ?it/s]mapping supervised_set surface forms...:   0%|          | 6/2731 [00:00<00:53, 50.63it/s]mapping supervised_set surface forms...:   1%|          | 30/2731 [00:00<00:25, 106.95it/s]mapping supervised_set surface forms...:   1%|‚ñè         | 40/2731 [00:00<00:26, 100.61it/s]mapping supervised_set surface forms...:   7%|‚ñã         | 186/2731 [00:00<00:10, 232.79it/s]mapping supervised_set surface forms...:   7%|‚ñã         | 204/2731 [00:01<00:17, 147.24it/s]mapping supervised_set surface forms...:   8%|‚ñä         | 227/2731 [00:01<00:18, 134.42it/s]mapping supervised_set surface forms...:   9%|‚ñâ         | 239/2731 [00:01<00:21, 113.58it/s]mapping supervised_set surface forms...:   9%|‚ñâ         | 249/2731 [00:02<00:27, 90.76it/s] mapping supervised_set surface forms...:  11%|‚ñà         | 302/2731 [00:02<00:15, 154.83it/s]mapping supervised_set surface forms...:  14%|‚ñà‚ñé        | 374/2731 [00:02<00:09, 251.73it/s]mapping supervised_set surface forms...:  16%|‚ñà‚ñã        | 448/2731 [00:02<00:06, 337.34it/s]mapping supervised_set surface forms...:  18%|‚ñà‚ñä        | 494/2731 [00:02<00:08, 272.65it/s]mapping supervised_set surface forms...:  24%|‚ñà‚ñà‚ñç       | 659/2731 [00:02<00:03, 529.79it/s]mapping supervised_set surface forms...:  27%|‚ñà‚ñà‚ñã       | 735/2731 [00:02<00:03, 538.93it/s]mapping supervised_set surface forms...:  32%|‚ñà‚ñà‚ñà‚ñè      | 883/2731 [00:02<00:02, 744.00it/s]mapping supervised_set surface forms...:  36%|‚ñà‚ñà‚ñà‚ñå      | 977/2731 [00:03<00:02, 760.45it/s]mapping supervised_set surface forms...:  39%|‚ñà‚ñà‚ñà‚ñâ      | 1067/2731 [00:03<00:02, 557.37it/s]mapping supervised_set surface forms...:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1140/2731 [00:03<00:03, 430.75it/s]mapping supervised_set surface forms...:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1353/2731 [00:03<00:01, 713.97it/s]mapping supervised_set surface forms...:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1456/2731 [00:03<00:01, 697.74it/s]mapping supervised_set surface forms...:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1547/2731 [00:04<00:02, 468.03it/s]mapping supervised_set surface forms...:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1694/2731 [00:04<00:01, 596.59it/s]mapping supervised_set surface forms...:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1893/2731 [00:04<00:00, 838.26it/s]mapping supervised_set surface forms...:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2012/2731 [00:04<00:01, 696.40it/s]mapping supervised_set surface forms...:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2177/2731 [00:04<00:00, 850.24it/s]mapping supervised_set surface forms...:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2290/2731 [00:05<00:01, 316.33it/s]mapping supervised_set surface forms...:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2373/2731 [00:06<00:01, 352.74it/s]mapping supervised_set surface forms...:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2563/2731 [00:06<00:00, 523.97it/s]mapping supervised_set surface forms...:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2702/2731 [00:06<00:00, 642.09it/s]mapping supervised_set surface forms...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2731/2731 [00:06<00:00, 437.61it/s]
reading surface forms from ontology.json:   0%|          | 0/31 [00:00<?, ?it/s]reading surface forms from ontology.json:  29%|‚ñà‚ñà‚ñâ       | 9/31 [00:00<00:00, 77.76it/s]reading surface forms from ontology.json:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 19/31 [00:00<00:00, 66.19it/s]reading surface forms from ontology.json:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 26/31 [00:02<00:00,  8.22it/s]reading surface forms from ontology.json:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 [00:02<00:00,  8.02it/s]reading surface forms from ontology.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:02<00:00, 10.74it/s]
INFO 08-28 23:24:27 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 08-28 23:24:29 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 400, in <module>
[rank0]:     main(**args)
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 285, in main
[rank0]:     experiment: CodexExperiment = CodexExperiment(
[rank0]:                                   ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 89, in __init__
[rank0]:     self.codex_client = LlamaClient(engine=codex_engine, stop_sequences=STOP_SEQUENCES.get(self.prompt_format), beam_search_config=self.beam_search_config)
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/refpydst/codex_client.py", line 257, in __init__
[rank0]:     self.model = LLM(model=self.engine, quantization=quantization, enforce_eager=True)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 155, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 441, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 251, in __init__
[rank0]:     self.model_executor = executor_class(
[rank0]:                           ^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 47, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 36, in _init_executor
[rank0]:     self.driver_worker.load_model()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[rank0]:     self.model = get_model(model_config=self.model_config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[rank0]:     return loader.load_model(model_config=model_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[rank0]:     model = _initialize_model(model_config, self.load_config,
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[rank0]:     return model_class(config=model_config.hf_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[rank0]:     self.model = LlamaModel(config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(
[rank0]:                                                     ^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [
[rank0]:                                                      ^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[rank0]:     lambda prefix: LlamaDecoderLayer(config=config,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[rank0]:     self.mlp = LlamaMLP(
[rank0]:                ^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[rank0]:     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[rank0]:     self.quant_method.create_weights(
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 400, in <module>
[rank0]:     main(**args)
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 285, in main
[rank0]:     experiment: CodexExperiment = CodexExperiment(
[rank0]:                                   ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 89, in __init__
[rank0]:     self.codex_client = LlamaClient(engine=codex_engine, stop_sequences=STOP_SEQUENCES.get(self.prompt_format), beam_search_config=self.beam_search_config)
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/refpydst/codex_client.py", line 257, in __init__
[rank0]:     self.model = LLM(model=self.engine, quantization=quantization, enforce_eager=True)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 155, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 441, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 251, in __init__
[rank0]:     self.model_executor = executor_class(
[rank0]:                           ^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 47, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 36, in _init_executor
[rank0]:     self.driver_worker.load_model()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 139, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 682, in load_model
[rank0]:     self.model = get_model(model_config=self.model_config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 21, in get_model
[rank0]:     return loader.load_model(model_config=model_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 280, in load_model
[rank0]:     model = _initialize_model(model_config, self.load_config,
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 111, in _initialize_model
[rank0]:     return model_class(config=model_config.hf_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 384, in __init__
[rank0]:     self.model = LlamaModel(config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 285, in __init__
[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(
[rank0]:                                                     ^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 144, in make_layers
[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [
[rank0]:                                                      ^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/utils.py", line 145, in <listcomp>
[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 287, in <lambda>
[rank0]:     lambda prefix: LlamaDecoderLayer(config=config,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[rank0]:     self.mlp = LlamaMLP(
[rank0]:                ^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 76, in __init__
[rank0]:     self.down_proj = RowParallelLinear(input_size=intermediate_size,
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 728, in __init__
[rank0]:     self.quant_method.create_weights(
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py", line 109, in create_weights
[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/torch/utils/_device.py", line 78, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run -runs-instance_ad_hoc-8B-bm25-tf_idf at: https://wandb.ai/hacastle12/refpydst/runs/xb1rlpe9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240828_232411-xb1rlpe9/logs
