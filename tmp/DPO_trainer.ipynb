{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huijunkim/anaconda3/envs/ref/lib/python3.8/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, set_seed\n",
    "\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The arguments for the DPO training script.\n",
    "    \"\"\"\n",
    "\n",
    "    # data parameters\n",
    "    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n",
    "\n",
    "    # training parameters\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"../sft/results/final_checkpoint\",\n",
    "        metadata={\"help\": \"the location of the SFT model name or path\"},\n",
    "    )\n",
    "    learning_rate: Optional[float] = field(default=5e-4, metadata={\"help\": \"optimizer learning rate\"})\n",
    "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
    "    warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"the number of warmup steps\"})\n",
    "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
    "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
    "\n",
    "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"train batch size per device\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"eval batch size per device\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
    "    )\n",
    "\n",
    "    gradient_checkpointing_use_reentrant: Optional[bool] = field(\n",
    "        default=False, metadata={\"help\": \"whether to use reentrant for gradient checkpointing\"}\n",
    "    )\n",
    "\n",
    "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
    "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
    "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
    "\n",
    "    max_prompt_length: Optional[int] = field(default=512, metadata={\"help\": \"the maximum prompt length\"})\n",
    "    max_length: Optional[int] = field(default=1024, metadata={\"help\": \"the maximum sequence length\"})\n",
    "    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"max number of training steps\"})\n",
    "    logging_steps: Optional[int] = field(default=10, metadata={\"help\": \"the logging frequency\"})\n",
    "    save_steps: Optional[int] = field(default=100, metadata={\"help\": \"the saving frequency\"})\n",
    "    eval_steps: Optional[int] = field(default=100, metadata={\"help\": \"the evaluation frequency\"})\n",
    "\n",
    "    output_dir: Optional[str] = field(default=\"./results\", metadata={\"help\": \"the output directory\"})\n",
    "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=True, metadata={\"help\": \"whether to load the model in 4bit\"})\n",
    "    model_dtype: Optional[str] = field(\n",
    "        default=\"float16\", metadata={\"help\": \"model_dtype[float16, bfloat16, float] for loading.\"}\n",
    "    )\n",
    "\n",
    "    # instrumentation\n",
    "    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n",
    "    report_to: Optional[str] = field(\n",
    "        default=\"wandb\",\n",
    "        metadata={\n",
    "            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n",
    "            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n",
    "            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n",
    "        },\n",
    "    )\n",
    "    # debug argument for distributed training\n",
    "    ignore_bias_buffers: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n",
    "            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n",
    "        },\n",
    "    )\n",
    "    seed: Optional[int] = field(\n",
    "        default=0, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dpomw(\n",
    "    sanity_check: bool = False,\n",
    "    cache_dir: Optional[str] = None,\n",
    "    num_proc=24,\n",
    "    train: bool = True\n",
    ") -> Dataset:\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"anthj/dpo_mw\",\n",
    "        split= \"train\" if train else \"eval\",\n",
    "        verification_mode=\"no_checks\"\n",
    "    )\n",
    "    original_columns = dataset.column_names\n",
    "\n",
    "    def return_prompt_and_responses(samples) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n",
    "            \"chosen\": samples[\"chosen\"],\n",
    "            \"rejected\": samples[\"rejected\"],\n",
    "        }\n",
    "\n",
    "    return dataset.map(\n",
    "        return_prompt_and_responses,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_nofilter = get_dpomw(train=True)\n",
    "eval_dataset_nofilter = get_dpomw(train=False)\n",
    "\n",
    "train_dataset = train_dataset_nofilter.filter(\n",
    "        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= 1024\n",
    "        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= 1024\n",
    "    )\n",
    "eval_dataset = eval_dataset_nofilter.filter(\n",
    "        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= 1024\n",
    "        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= 1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train set, size before filtering: 1157, after filtering: 1112\n",
      "In eval set, size before filtering: 290, after filtering: 277\n"
     ]
    }
   ],
   "source": [
    "print(f\"In train set, size before filtering: {len(train_dataset_nofilter)}, after filtering: {len(train_dataset)}\")\n",
    "print(f\"In eval set, size before filtering: {len(eval_dataset_nofilter)}, after filtering: {len(eval_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
