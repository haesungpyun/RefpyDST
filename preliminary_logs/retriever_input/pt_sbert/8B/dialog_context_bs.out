/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
wandb: Currently logged in as: hacastle12. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.11
wandb: Run data is saved locally in /home/haesungpyun/my_refpydst/wandb/run-20240830_104007-gwiv8f2w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run -runs-preliminary-bm25-python_no_guidelines-greedy-8B-dialog_context_text
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hacastle12/refpydst
wandb: üöÄ View run at https://wandb.ai/hacastle12/refpydst/runs/gwiv8f2w
mapping supervised_set surface forms...:   0%|          | 0/2731 [00:00<?, ?it/s]mapping supervised_set surface forms...:   0%|          | 7/2731 [00:00<00:47, 57.84it/s]mapping supervised_set surface forms...:   1%|          | 31/2731 [00:00<00:20, 133.65it/s]mapping supervised_set surface forms...:   7%|‚ñã         | 186/2731 [00:00<00:06, 384.59it/s]mapping supervised_set surface forms...:   8%|‚ñä         | 217/2731 [00:00<00:09, 272.24it/s]mapping supervised_set surface forms...:   9%|‚ñâ         | 241/2731 [00:01<00:12, 203.11it/s]mapping supervised_set surface forms...:  10%|‚ñâ         | 260/2731 [00:01<00:14, 171.12it/s]mapping supervised_set surface forms...:  16%|‚ñà‚ñå        | 442/2731 [00:01<00:04, 466.46it/s]mapping supervised_set surface forms...:  19%|‚ñà‚ñä        | 511/2731 [00:01<00:05, 433.98it/s]mapping supervised_set surface forms...:  25%|‚ñà‚ñà‚ñç       | 671/2731 [00:01<00:03, 653.37it/s]mapping supervised_set surface forms...:  33%|‚ñà‚ñà‚ñà‚ñé      | 891/2731 [00:01<00:01, 987.02it/s]mapping supervised_set surface forms...:  38%|‚ñà‚ñà‚ñà‚ñä      | 1042/2731 [00:01<00:01, 1083.14it/s]mapping supervised_set surface forms...:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1172/2731 [00:02<00:01, 794.76it/s] mapping supervised_set surface forms...:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1443/2731 [00:02<00:01, 1172.42it/s]mapping supervised_set surface forms...:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1598/2731 [00:02<00:01, 687.42it/s] mapping supervised_set surface forms...:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1716/2731 [00:02<00:01, 701.61it/s]mapping supervised_set surface forms...:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1938/2731 [00:03<00:00, 852.87it/s]mapping supervised_set surface forms...:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2051/2731 [00:03<00:00, 827.25it/s]mapping supervised_set surface forms...:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2177/2731 [00:03<00:00, 896.24it/s]mapping supervised_set surface forms...:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2284/2731 [00:04<00:01, 304.53it/s]mapping supervised_set surface forms...:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2362/2731 [00:04<00:01, 333.40it/s]mapping supervised_set surface forms...:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 2555/2731 [00:04<00:00, 504.22it/s]mapping supervised_set surface forms...:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2686/2731 [00:04<00:00, 607.61it/s]mapping supervised_set surface forms...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2731/2731 [00:04<00:00, 574.31it/s]
reading surface forms from ontology.json:   0%|          | 0/31 [00:00<?, ?it/s]reading surface forms from ontology.json:  29%|‚ñà‚ñà‚ñâ       | 9/31 [00:00<00:00, 72.51it/s]reading surface forms from ontology.json:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 19/31 [00:00<00:00, 59.50it/s]reading surface forms from ontology.json:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 26/31 [00:02<00:00,  7.46it/s]reading surface forms from ontology.json:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 [00:03<00:00,  7.27it/s]reading surface forms from ontology.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:03<00:00,  9.73it/s]
INFO 08-30 10:40:21 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 08-30 10:40:22 model_runner.py:680] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...
INFO 08-30 10:40:23 weight_utils.py:223] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.91s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.53s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.72s/it]

INFO 08-30 10:40:31 model_runner.py:692] Loading model weights took 14.9595 GB
INFO 08-30 10:40:33 gpu_executor.py:102] # GPU blocks: 27955, # CPU blocks: 2048
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 360, in <module>
[rank0]:     main(**args)
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 290, in main
[rank0]:     experiment: CodexExperiment = CodexExperiment(
[rank0]:                                   ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 89, in __init__
[rank0]:     self.codex_client = LlamaClient(engine=codex_engine, stop_sequences=STOP_SEQUENCES.get(self.prompt_format), beam_search_config=self.beam_search_config)
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/refpydst/codex_client.py", line 257, in __init__
[rank0]:     self.model = LLM(model=self.engine, quantization=quantization, enforce_eager=True)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 155, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 441, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 265, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 377, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 105, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 219, in initialize_cache
[rank0]:     self._init_cache_engine()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 224, in _init_cache_engine
[rank0]:     self.cache_engine = [
[rank0]:                         ^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 225, in <listcomp>
[rank0]:     CacheEngine(self.cache_config, self.model_config,
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/cache_engine.py", line 66, in __init__
[rank0]:     self.gpu_cache = self._allocate_kv_cache(
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/cache_engine.py", line 85, in _allocate_kv_cache
[rank0]:     torch.zeros(kv_cache_shape,
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 360, in <module>
[rank0]:     main(**args)
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 290, in main
[rank0]:     experiment: CodexExperiment = CodexExperiment(
[rank0]:                                   ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/my_refpydst/src/refpydst/run_codex_experiment.py", line 89, in __init__
[rank0]:     self.codex_client = LlamaClient(engine=codex_engine, stop_sequences=STOP_SEQUENCES.get(self.prompt_format), beam_search_config=self.beam_search_config)
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/refpydst/codex_client.py", line 257, in __init__
[rank0]:     self.model = LLM(model=self.engine, quantization=quantization, enforce_eager=True)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 155, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 441, in from_engine_args
[rank0]:     engine = cls(
[rank0]:              ^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 265, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 377, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/executor/gpu_executor.py", line 105, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 219, in initialize_cache
[rank0]:     self._init_cache_engine()
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 224, in _init_cache_engine
[rank0]:     self.cache_engine = [
[rank0]:                         ^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/worker.py", line 225, in <listcomp>
[rank0]:     CacheEngine(self.cache_config, self.model_config,
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/cache_engine.py", line 66, in __init__
[rank0]:     self.gpu_cache = self._allocate_kv_cache(
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/haesungpyun/anaconda3/envs/llama/lib/python3.11/site-packages/vllm/worker/cache_engine.py", line 85, in _allocate_kv_cache
[rank0]:     torch.zeros(kv_cache_shape,
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: \ 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: - 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: üöÄ View run -runs-preliminary-bm25-python_no_guidelines-greedy-8B-dialog_context_text at: https://wandb.ai/hacastle12/refpydst/runs/gwiv8f2w
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240830_104007-gwiv8f2w/logs
