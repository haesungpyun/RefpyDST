{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_completion(prompts, args, llm):\n",
    "    config = SamplingParams(temperature=0, max_tokens=args.max_new_tokens)\n",
    "    if \"llama\" in args.llm.lower():\n",
    "        prompts = [LLAMA3_CHAT.format(PROMPT=p) for p in prompts]\n",
    "    else:\n",
    "        prompts = [QWEN_CHAT.format(PROMPT=p) for p in prompts]\n",
    "    output= llm.generate(prompts=prompts, sampling_params=config)\n",
    "    output = [o.outputs[0].text.strip() for o in output]\n",
    "    if \"llama\" in args.llm.lower():\n",
    "        output = [o.split(\"<|eot_id|>\")[0] for o in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model='meta-llama/Meta-Llama-3-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example shows how to use LoRA with different quantization techniques\n",
    "for offline inference.\n",
    "\n",
    "Requires HuggingFace credentials for access.\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "\n",
    "\"\"\"Main function that sets up and runs the prompt processing.\"\"\"\n",
    "test_config= {\n",
    "\"name\": \"qlora_inference_example\",\n",
    "'model': \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "'quantization': \"bitsandbytes\",\n",
    "'lora_repo': 'timdettmers/qlora-flan-7b'\n",
    "}\n",
    "\n",
    "engine_args = EngineArgs(\n",
    "    model=test_config['model'],\n",
    "    quantization=test_config['quantization'],\n",
    "    qlora_adapter_name_or_path=test_config['lora_repo'],\n",
    "    load_format=test_config['quantization'],\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    # set it only in GPUs of limited memory\n",
    "    enforce_eager=True)\n",
    "engine = LLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "lora_path = snapshot_download(repo_id=test_config['lora_repo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "     ('Say Hello to me',SamplingParams(temperature=0.0,\n",
    "                logprobs=1,\n",
    "                prompt_logprobs=1,\n",
    "                max_tokens=128), None),\n",
    "    # (\"my name is\", SamplingParams(temperature=0.0,\n",
    "    #             logprobs=1,\n",
    "    #             prompt_logprobs=1,\n",
    "    #             max_tokens=128),\n",
    "    # LoRARequest(\"lora-test-1\", 1, lora_path))\n",
    "    ]\n",
    "request_id = 0\n",
    "while test_prompts or engine.has_unfinished_requests():\n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request = test_prompts.pop(0)\n",
    "            engine.add_request(str(request_id),\n",
    "                               prompt,\n",
    "                               sampling_params,\n",
    "                               lora_request=lora_request)\n",
    "            request_id += 1\n",
    "\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                print(\"----------------------------------------------------\")\n",
    "                print(f\"Prompt: {request_output.prompt}\")\n",
    "                print(f\"Output: {request_output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
